{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vital-advertising",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "# Fetch Swedish data on your own \n",
    "---\n",
    "\n",
    "## Due to GDPR , we are not allowed to provide data to attendees in the bootcamp !\n",
    "- **Therefore, we kindly ask you to fetch and preprocess the data on your own following the steps given below-  **\n",
    "    - wget språkbank [webnyheter2013](http://spraakbanken.gu.se/lb/resurser/meningsmangder/webbnyheter2013.xml.bz2)\n",
    "    - wget språkbank [provided script](https://raw.githubusercontent.com/spraakbanken/sb-nltk-tools/master/sb_corpus_reader.py) to extract the data\n",
    "    - use the function below to extract the xml file into raw txt file\n",
    "    - (additional advice) discuss filtering on number of sentences per document and number of tokens per sentence \n",
    "        - example for English GPT training, it is recommand to check on the stats of your raw data and come-up with a good rule-of-thumb to proceed filtering, it is,however,recommanded to look into language-specific cleaning and follow up robust clearning procedure to obtain quality corpus.\n",
    "\n",
    "in this notebook, we will embrace the method provided by the [Megatron-LM repo](https://github.com/NVIDIA/Megatron-LM/tree/main/tools/openwebtext) as well as introduce other complimenting methods that might be of interest for data cleaning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-reception",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------\n",
    "#### fetch the webnyheter2013 data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "steady-henry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-08-27 02:36:06--  http://spraakbanken.gu.se/lb/resurser/meningsmangder/webbnyheter2013.xml.bz2\n",
      "Resolving spraakbanken.gu.se (spraakbanken.gu.se)... 130.241.42.13\n",
      "Connecting to spraakbanken.gu.se (spraakbanken.gu.se)|130.241.42.13|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://spraakbanken.gu.se/lb/resurser/meningsmangder/webbnyheter2013.xml.bz2 [following]\n",
      "--2021-08-27 02:36:06--  https://spraakbanken.gu.se/lb/resurser/meningsmangder/webbnyheter2013.xml.bz2\n",
      "Connecting to spraakbanken.gu.se (spraakbanken.gu.se)|130.241.42.13|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 464382665 (443M) [application/x-bzip2]\n",
      "Saving to: ‘webbnyheter2013.xml.bz2’\n",
      "\n",
      "webbnyheter2013.xml 100%[===================>] 442.87M  16.9MB/s    in 28s     \n",
      "\n",
      "2021-08-27 02:36:35 (16.0 MB/s) - ‘webbnyheter2013.xml.bz2’ saved [464382665/464382665]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://spraakbanken.gu.se/lb/resurser/meningsmangder/webbnyheter2013.xml.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "exposed-mouth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bunzip2: Input file ../../../../dataset/SV/ is a directory.\n"
     ]
    }
   ],
   "source": [
    "!bunzip2 -d webbnyheter2013.xml.bz2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "turned-navigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./webbnyheter2013.xml ../../../../dataset/SV/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "level-discipline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56k  webbnyheter2013.xml\n"
     ]
    }
   ],
   "source": [
    "!ls ../../../../dataset/SV/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "separated-payday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-08-27 03:35:01--  https://raw.githubusercontent.com/spraakbanken/sb-nltk-tools/master/sb_corpus_reader.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3065 (3.0K) [text/plain]\n",
      "Saving to: ‘sb_corpus_reader.py’\n",
      "\n",
      "sb_corpus_reader.py 100%[===================>]   2.99K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-08-27 03:35:01 (45.7 MB/s) - ‘sb_corpus_reader.py’ saved [3065/3065]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/spraakbanken/sb-nltk-tools/master/sb_corpus_reader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "guilty-comparative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fname:  .txt\n",
      "[['Telekombranschen', 'lyfts', 'av', 'en', 'större', 'europeisk', 'telekomaffär', ',', 'nederländska', 'KPN', 'säljer', 'tysk', 'verksamhet', 'för', 'omkring', 'åtta', 'miljarder', 'euro', ',', 'och', 'en', 'stark', 'rapport', 'från', 'Telenor', '.'], ['Denna', 'upprepade', 'process', 'är', 'död', 'nu', '\"', ',', 'skriver', '\"', 'Shield', '\"', '-', 'skaparen', 'Shawn', 'Ryan', ',', 'som', 'låg', 'bakom', 'idén', ',', 'på', 'Twitter', '.']]\n",
      "write to :  webnyheter2013.txt\n",
      "finish processing  webnyheter2013.txt\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sb_corpus_reader import SBCorpusReader\n",
    "import random\n",
    "\n",
    "def write2csv(out_path, fname, sents):\n",
    "    f=open(out_path+fname,'a')\n",
    "    for s in sents:\n",
    "        if len(s)>=2:\n",
    "            s_text=' '.join(s)\n",
    "            f.write(s_text+'\\n')\n",
    "    print(\"finish processing \",fname)\n",
    "    f.close()\n",
    "    \n",
    "out_path='./dataset/SV/'\n",
    "xml_f=out_path+'webbnyheter2013.xml'\n",
    "if xml_f.endswith('.xml') :    \n",
    "    corpus = SBCorpusReader(xml_f)\n",
    "    sents=corpus.sents()\n",
    "    print(sents[:2])\n",
    "    #n=len(sents)\n",
    "    #rn=random.randint(0,n-1)\n",
    "    #print(\"a random sample of sentence : \\n\".format(' '.join(sents[rn])))\n",
    "    fname='webnyheter2013.txt'  \n",
    "    print(\"write to : \",fname)\n",
    "    write2csv(out_path,fname,sents)\n",
    "    print('-----'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "rubber-finnish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56k  webbnyheter2013.xml  webnyheter2013.txt\n"
     ]
    }
   ],
   "source": [
    "!ls ../../../../dataset/SV/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-somewhere",
   "metadata": {},
   "source": [
    "---\n",
    "## Up Next : \n",
    "\n",
    "[Find sentence boundary and deduplicate your data](./Day3-2_SentenceBoundary_and_Deduplicate.ipynb)\n",
    "\n",
    "## Back To Start Menu\n",
    "[start menu](../../../../Start_Here.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-appreciation",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "\n",
    "## Licensing \n",
    "\n",
    "This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
