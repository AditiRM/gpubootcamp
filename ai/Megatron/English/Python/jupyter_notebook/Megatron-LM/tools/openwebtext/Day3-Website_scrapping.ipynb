{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "grave-judges",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "# (option) : get your own data via scrap the website you have permisson to use\n",
    "\n",
    "note1 : strongly recommand to consult with your own legal department for compliance before proceeding this step on websites/webpages you have permission to\n",
    "\n",
    "note2 : modification needed when applying to different website/webpages\n",
    "\n",
    "note3 : use at your own risk !\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "- **The goal of this lab is to demonstrate that there are ways to obtain your own data via webscrapping sites which you have permission to**\n",
    "    - Provide a list of urls in a text file via collecting webpages links\n",
    "    \n",
    "    the base url used to crawl links from is  https://developer.nvidia.com/blog/\n",
    "    \n",
    "    - scrap the webpage ( in html format ) using [scrapy](https://docs.scrapy.org/en/latest/index.html) and obtain desired text, concatenate text per webpage into one raw text file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-steering",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install html5lib\n",
    "!pip install PyPDF2\n",
    "!pip install selenium\n",
    "!pip install Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-brooklyn",
   "metadata": {},
   "source": [
    "## crawl NVblog landing page and obtain links to the individual blogs\n",
    "source github repo : https://github.com/x4nth055/pythoncode-tutorials/tree/master/web-scraping/link-extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "interstate-kernel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-09-15 09:14:55--  https://raw.githubusercontent.com/x4nth055/pythoncode-tutorials/master/web-scraping/link-extractor/link_extractor.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3485 (3.4K) [text/plain]\n",
      "Saving to: ‘link_extractor.py’\n",
      "\n",
      "link_extractor.py   100%[===================>]   3.40K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2021-09-15 09:14:56 (3.97 MB/s) - ‘link_extractor.py’ saved [3485/3485]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/x4nth055/pythoncode-tutorials/master/web-scraping/link-extractor/link_extractor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "extraordinary-enforcement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-09-15 09:14:58--  https://raw.githubusercontent.com/x4nth055/pythoncode-tutorials/master/web-scraping/link-extractor/link_extractor_js.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3727 (3.6K) [text/plain]\n",
      "Saving to: ‘link_extractor_js.py’\n",
      "\n",
      "link_extractor_js.p 100%[===================>]   3.64K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2021-09-15 09:14:58 (5.71 MB/s) - ‘link_extractor_js.py’ saved [3727/3727]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/x4nth055/pythoncode-tutorials/master/web-scraping/link-extractor/link_extractor_js.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests bs4 colorama requests-html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "american-offset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[*] Crawling: https://blogs.nvidia.com/blog/category/deep-learning/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/category/deep-learning/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/autonomous-machines/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/autonomous-machines/uavs-drones-technology/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/autonomous-machines/robotics/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/autonomous-machines/intelligent-video-analytics-platform/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-nano/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-xavier-nx/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-agx-xavier/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-tx2/\u001b[39m\n",
      "\u001b[90m[!] External link: https://developer.nvidia.com/embedded-computing\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/autonomous-machines/jetson-store/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/data-center/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/data-center/products/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/data-center/tesla\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/data-center/products/enterprise-server/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/data-center/dgx-1/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/data-center/dgx-2/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/data-center/dgx-station/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/data-center/hgx\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/gpu-cloud\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/data-center/virtual-solutions/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/data-center/products/egx-edge-computing/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/data-center/solutions/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/deep-learning-ai/solutions/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/high-performance-computing/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/data-center/rtx-server-gaming/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/data-center/gpu-cloud-computing/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/data-center/virtualization/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/data-center/ai-accelerated-analytics/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/gpu-accelerated-applications/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/data-center/gpu-accelerated-applications/\u001b[39m\n",
      "\u001b[90m[!] External link: https://developer.nvidia.com/hpc\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/technologies/cuda-x/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/data-center/magnum-io/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/data-center/volta-gpu-architecture/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/data-center/pascal-gpu-architecture/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/data-center/nvlink/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/data-center/tensorcore/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/data-center/index-paraview-plugin/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/data-center/virtual-gpu-technology/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/deep-learning-ai/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/deep-learning-ai/industries/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/deep-learning-ai/industries/ai-innovators/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/deep-learning-ai/industries/ai-cities/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/deep-learning-ai/industries/public-good/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/deep-learning-ai/industries/healthcare/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/deep-learning-ai/industries/higher-education-research/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/deep-learning-ai/industries/retail/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/deep-learning-ai/industries/robotics/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/deep-learning-ai/industries/automotive/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/deep-learning-ai/developer/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/deep-learning-ai/products/agx-systems/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/data-center/dgx-systems/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/gpu-cloud/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/titan/titan-rtx/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/data-center/tesla/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/deep-learning-ai/solutions/inference-platform/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/deep-learning-ai/education/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/deep-learning-ai/startups/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/design-visualization/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/design-visualization/industries/architecture-engineering-construction\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/design-visualization/industries/education\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/design-visualization/industries/manufacturing\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/design-visualization/industries/media-entertainment\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/design-visualization/quadro/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/design-visualization/quadro-vdws/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/design-visualization/grid-vpc-vapps/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/design-visualization/nvs-graphics-cards/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/design-visualization/solutions/quadro-display-desktop-management/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/design-visualization/solutions/rendering/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/design-visualization/solutions/virtualization/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/design-visualization/solutions/quadro-vr/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/design-visualization/creators/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/workstations/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/design-visualization/technologies/material-definition-language/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/design-visualization/technologies/rtx/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/design-visualization/technologies/turing-architecture/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/design-visualization/technologies/virtual-gpu/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/design-visualization/technologies/holodeck/\u001b[39m\n",
      "\u001b[90m[!] External link: https://developer.nvidia.com/designworks\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/design-visualization/quadro-store/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/healthcare/\u001b[39m\n",
      "\u001b[90m[!] External link: https://developer.nvidia.com/clara-medical-imaging\u001b[39m\n",
      "\u001b[90m[!] External link: https://developer.nvidia.com/Clara-Genomics\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/data-center/gpu-accelerated-applications/catalog/\u001b[39m\n",
      "\u001b[90m[!] External link: https://developer.nvidia.com/cuda-zone\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/self-driving-cars/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/self-driving-cars/drive-px\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/self-driving-cars/data-center/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/self-driving-cars/drive-constellation/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/self-driving-cars/drive-ix/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/self-driving-cars/hd-mapping\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/self-driving-cars/adas\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/self-driving-cars/partners\u001b[39m\n",
      "\u001b[90m[!] External link: https://developer.nvidia.com/drive\u001b[39m\n",
      "\u001b[90m[!] External link: http://www.geforce.com/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/geforce/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/geforce/20-series/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/geforce/graphics-cards/16-series/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/geforce/gaming-laptops/20-series/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/geforce/products/g-sync-monitors/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/geforce-now/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/shield/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/geforce/products/big-format-gaming-displays/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/geforce/20-series/rtx-for-broadcasting/\u001b[39m\n",
      "\u001b[90m[!] External link: https://developer.nvidia.com/\u001b[39m\n",
      "\u001b[90m[!] External link: https://news.developer.nvidia.com\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://devblogs.nvidia.com/\u001b[39m\n",
      "\u001b[90m[!] External link: https://devtalk.nvidia.com/\u001b[39m\n",
      "\u001b[90m[!] External link: https://developer.nvidia.com/open-source\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/gtc/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/industries/game-development/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/industries/healthcare/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/industries/higher-education-research/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/industries/industrial/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/industries/media-and-entertainment/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/industries/public-sector/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/industries/retail/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/industries/supercomputing/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/industries/telecommunications/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/industries/transportation/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/industries/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/shop/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.geforce.com/drivers\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/Download/index.aspx\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/support/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/about-nvidia/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/about-nvidia/partners/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/about-nvidia/ai-computing/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/technologies/\u001b[39m\n",
      "\u001b[90m[!] External link: https://nvidianews.nvidia.com/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/research/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/about-nvidia/webinar-portal/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/about-nvidia/events/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/about-nvidia/careers/\u001b[39m\n",
      "\u001b[90m[!] External link: https://investor.nvidia.com/home/default.aspx\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/csr/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/foundation/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/preferences/email-signup/\u001b[39m\n",
      "\u001b[90m[!] External link: https://twitter.com/nvidia\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.facebook.com/NVIDIA\u001b[39m\n",
      "\u001b[90m[!] External link: https://news.ycombinator.com/submit\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.instagram.com/nvidia/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.linkedin.com/company/nvidia/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.reddit.com/r/nvidia/\u001b[39m\n",
      "\u001b[90m[!] External link: https://feeds.feedburner.com/nvidiablog\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.twitch.tv/nvidia\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.youtube.com/nvidia\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/privacy-commenting-policy/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/about-nvidia/legal-info/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/contact/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/category/enterprise/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/category/auto/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/category/gaming/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/category/pro-graphics/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/category/autonomous-machines/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/tag/medical-research-and-healthcare/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/tag/inception/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/ai-podcast/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/2021/09/14/university-of-florida-rankings-ai/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/2021/09/07/cloudera-spark-irs-gpus/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/2021/09/02/johnson-and-johnson-domino-data-science-mlops/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/2021/09/13/geforce-now-highlights-freestyle-montage/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/2021/09/10/iaa-mobility-auto-show/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/2021/09/09/geforce-now-thursday-september-09/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/2021/09/08/drive-labs-ai-based-live-perception/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/2021/09/08/gantheftauto/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/forums/\u001b[39m\n",
      "\u001b[90m[!] External link: https://developer.nvidia.com/developer-program\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/deep-learning-ai/startups/venture-capital/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/about-nvidia/gpu-ventures/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/training/\u001b[39m\n",
      "\u001b[90m[!] External link: https://academy.mellanox.com/en/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/ai-data-science/professional-services/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/events/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/on-demand/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/contact/social/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/location-selector/\u001b[39m\n",
      "\u001b[90m[!] External link: https://twitter.com/intent/tweet\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.facebook.com/dialog/share\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/wp-admin/admin-ajax.php\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.linkedin.com/shareArticle\u001b[39m\n",
      "\u001b[33m[*] Crawling: https://blogs.nvidia.com/wp-admin/admin-ajax.php\u001b[39m\n",
      "\u001b[33m[*] Crawling: https://blogs.nvidia.com/blog/2021/09/14/university-of-florida-rankings-ai/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/author/rickmerritt/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.facebook.com/sharer/sharer.php\u001b[39m\n",
      "\u001b[90m[!] External link: http://news.ycombinator.com/submitlink\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.usnews.com/best-colleges/rankings/national-universities/top-public\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.eng.ufl.edu/newengineer/in-the-headlines/provost-joe-glover-ufs-new-ai-supercomputer-will-change-how-students-learn/\u001b[39m\n",
      "\u001b[90m[!] External link: https://johnkoetsier.com/hipergator-ai-supercomputer-florida/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/2020/07/21/university-of-florida-nvidia-ai-supercomputer/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/category/enterprise/supercomputing/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/tag/artificial-intelligence/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/tag/high-performance-computing/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/tag/machine-learning/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/tag/nvidia-a100/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/tag/nvidia-dgx/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/tag/science/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/tag/supercomputing/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/gtc/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/2021/09/14/gpu-train-invisible-keyboard/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/2021/09/10/ai-revolution-in-africa/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/2021/09/09/gpu-alzheimers/\u001b[39m\n",
      "[+] Total Internal links: 34\n",
      "[+] Total External links: 171\n",
      "[+] Total URLs: 205\n",
      "[+] Total crawled URLs: 2\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python link_extractor_js.py https://blogs.nvidia.com/blog/category/deep-learning/ -m 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "blind-opportunity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as request\n",
    "import re\n",
    "import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-firmware",
   "metadata": {},
   "source": [
    "## fetch the urls of interest and convert to html files \n",
    "### noted the url in NVdevblog_url.txt has been pre-filtered to ensure only NVblog pages are scrapped !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "mighty-treat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('predicting_protein_structures_with_deep_learning.html',\n",
       " 'https://developer.nvidia.com/blog/predicting-protein-structures-with-deep-learning/')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import os, sys\n",
    "# create folder to hold the scrapped html pages\n",
    "os.makedirs('./htmls/', exist_ok=True)\n",
    "f=open('NVdevblog_urls.txt','r')\n",
    "lines=f.readlines()\n",
    "rn=random.randint(0,len(lines)-1)\n",
    "url=str(lines[rn]).strip()\n",
    "name=url.split(':')[1][2:].split('/')[-2].replace('-','_')+'.html'\n",
    "name=str(name)\n",
    "name, url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-court",
   "metadata": {},
   "source": [
    "## fetch given url and save as .html file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "authorized-convertible",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import scrapy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "multiple-supervision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./htmls/response_0.html\n",
      "./htmls/response_1.html\n",
      "./htmls/response_2.html\n",
      "./htmls/response_3.html\n",
      "./htmls/response_4.html\n",
      "./htmls/response_5.html\n",
      "./htmls/response_6.html\n",
      "./htmls/response_7.html\n",
      "./htmls/response_8.html\n",
      "./htmls/response_9.html\n",
      "./htmls/response_10.html\n",
      "./htmls/response_11.html\n",
      "./htmls/response_12.html\n",
      "./htmls/response_13.html\n",
      "./htmls/response_14.html\n",
      "./htmls/response_15.html\n",
      "./htmls/response_16.html\n",
      "./htmls/response_17.html\n",
      "./htmls/response_18.html\n",
      "./htmls/response_19.html\n",
      "./htmls/response_20.html\n",
      "./htmls/response_21.html\n",
      "./htmls/response_22.html\n",
      "./htmls/response_23.html\n",
      "./htmls/response_24.html\n",
      "./htmls/response_25.html\n",
      "./htmls/response_26.html\n",
      "./htmls/response_27.html\n",
      "./htmls/response_28.html\n",
      "./htmls/response_29.html\n",
      "./htmls/response_30.html\n",
      "./htmls/response_31.html\n",
      "./htmls/response_32.html\n",
      "./htmls/response_33.html\n",
      "./htmls/response_34.html\n",
      "./htmls/response_35.html\n",
      "./htmls/response_36.html\n",
      "./htmls/response_37.html\n",
      "./htmls/response_38.html\n",
      "./htmls/response_39.html\n",
      "./htmls/response_40.html\n",
      "./htmls/response_41.html\n",
      "./htmls/response_42.html\n",
      "./htmls/response_43.html\n",
      "./htmls/response_44.html\n",
      "./htmls/response_45.html\n",
      "./htmls/response_46.html\n",
      "./htmls/response_47.html\n",
      "./htmls/response_48.html\n",
      "./htmls/response_49.html\n",
      "./htmls/response_50.html\n",
      "./htmls/response_51.html\n",
      "./htmls/response_52.html\n",
      "./htmls/response_53.html\n",
      "./htmls/response_54.html\n",
      "./htmls/response_55.html\n",
      "./htmls/response_56.html\n",
      "./htmls/response_57.html\n",
      "./htmls/response_58.html\n",
      "./htmls/response_59.html\n",
      "./htmls/response_60.html\n",
      "./htmls/response_61.html\n",
      "./htmls/response_62.html\n",
      "./htmls/response_63.html\n",
      "./htmls/response_64.html\n",
      "./htmls/response_65.html\n",
      "./htmls/response_66.html\n",
      "./htmls/response_67.html\n",
      "./htmls/response_68.html\n",
      "./htmls/response_69.html\n",
      "./htmls/response_70.html\n"
     ]
    }
   ],
   "source": [
    "!bash fetchURLs_and_write2html.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "offensive-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "import html5lib\n",
    "import codecs\n",
    "import os,sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "atmospheric-bangladesh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish processing htmls files and convert them to raw txt file\n"
     ]
    }
   ],
   "source": [
    "# read the html into python\n",
    "\n",
    "def covert2txt(html_f ,f_out):\n",
    "    file = codecs.open(html_f, \"r\", \"utf-8\")\n",
    "    html_doc=file.read()\n",
    "    soup = BeautifulSoup(html_doc)\n",
    "    sent_cnt=0\n",
    "    for node in soup.findAll('p'):\n",
    "        #print(type(node.text), node.text)\n",
    "        if node.text not in ['/n','','\\t',' ','\\n\\r'] : \n",
    "            sent_cnt+=1\n",
    "            f_out.write(node.text)            \n",
    "    f_out.write('\\n')      \n",
    "html_dir='./htmls/'\n",
    "htmls=os.listdir('./htmls')\n",
    "f_out=open('extractedNVblogs.txt' , 'a')\n",
    "for html in htmls:\n",
    "    outtxt=html.split('.')[0]   \n",
    "    covert2txt(html_dir+html ,f_out)\n",
    "f_out.close()\n",
    "print(\"finish processing htmls files and convert them to raw txt file\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-cabinet",
   "metadata": {},
   "source": [
    "---\n",
    "# move the extractedNVblogs.txt to the dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "minus-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv extractedNVblogs.txt ../../../../dataset/EN/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-accountability",
   "metadata": {},
   "source": [
    "---\n",
    "## clearn up the htmls folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fresh-thirty",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -fr htmls*\n",
    "!rm link_extractor.py\n",
    "!rm link_extractor_js.py\n",
    "!rm blogs.nvidia.com_*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-tamil",
   "metadata": {},
   "source": [
    "--- \n",
    "# doucle check the raw text file looks oki "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "front-batman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The NVIDIA NGC team is hosting a webinar with live Q&A to dive into this Jupyter notebook available from the NGC catalog. Learn how to use these resources to kickstart your AI journey. Register now: NVIDIA NGC Jupyter Notebook Day: Medical Imaging Segmentation.Image segmentation partitions a digital image into multiple segments by changing the representation into something more meaningful and easier to analyze. In the field of medical imaging, image segmentation can be used to help identify organs and anomalies, measure them, classify them, and even uncover diagnostic information. It does this by using data gathered from x-rays, magnetic resonance imaging (MRI), computed tomography (CT), positron emission tomography (PET), and other formats.To achieve state-of-the-art models that deliver the desired accuracy and performance for a use case, you must set up the right environment, train with the ideal hyperparameters, and optimize it to achieve the desired accuracy. All of this can be time-consuming. Data scientists and developers need the right set of tools to quickly overcome tedious tasks. That’s why we built the NGC catalog.The NGC catalog is a hub of GPU-optimized AI and HPC applications and tools. NGC provides easy access to performance-optimized containers, shortens model development time with pretrained models, and provides industry-specific SDKs to help build complete AI solutions and speed up AI workflows. These diverse assets can be used for a variety of use cases, ranging from computer vision and speech recognition to language understanding. The potential solutions span industries such as automotive, healthcare, manufacturing, and retail.In this post, we show how you can use the Medical 3D Image Segmentation notebook to predict brain tumors in MRI images. This post is suitable for anyone who is new to AI and has a particular interest in image segmentation as it applies to medical imaging. 3D U-Net enables the seamless segmentation of 3D volumes, with high accuracy and performance. It can be adapted to solve many different segmentation problems. Figure 2 shows that 3D U-Net consists of a contractive (left) and expanding (right) path. It repeatedly applies unpadded convolutions followed by max pooling for downsampling.In deep learning, a convolutional neural network (CNN) is a subset of deep neural networks, mostly used in image recognition and image processing. CNNs use deep learning to perform both generative and descriptive tasks, often using machine vision along with recommender systems and natural language processing.Padding in CNNs refers to the number of pixels added to an image when it is processed by the kernel of a CNN. Unpadded CNNs means that no pixels are added to the image.Pooling is a downsampling approach in CNN. Max pooling is one of common pooling methods that summarize the most activated presence of a feature. Every step in the expanding path consists of a feature map upsampling and a concatenation with the correspondingly cropped feature map from the contractive path.This resource contains a Dockerfile that extends the TensorFlow NGC container and encapsulates some dependencies. The resource can be downloaded using the following commands:wget --content-disposition https://api.ngc.nvidia.com/v2/resources/nvidia/unet3d_medical_for_tensorflow/versions/20.06.0/zip -O unet3d_medical_for_tensorflow_20.06.0.zipAside from these dependencies, you also need the following components:To train your model using mixed or TF32 precision with Tensor Cores or using FP32, perform the following steps using the default parameters of the 3D U-Net model on the Brain Tumor Segmentation 2019 dataset.Download the resource manually by clicking the three dots at the top-right corner of the resource page.You could also use the following wget command:wget --content-disposition https://api.ngc.nvidia.com/v2/resources/nvidia/unet3d_medical_for_tensorflow/versions/20.06.0/zip -O unet3d_medical_for_tensorflow_20.06.0.zipThis command uses the Dockerfile to create a Docker image named unet3d_tf, downloading all the required components automatically.docker build -t unet3d_tf .Data can be obtained by registering on the Brain Tumor Segmentation 2019 dataset website. The data should be downloaded and placed where /data in the container is mounted.To start an interactive session in the NGC container to run preprocessing, training, and inference, you must run the following command. This launches the container and mounts the ./data directory as a volume to the /data directory inside the container, mounts the ./results directory to the /results directory in the container.The advantage of using a container is that it packages all the necessary libraries and dependencies into a single, isolated environment. This way you don’t have to worry about the complex install process.Use this command to start a Jupyter notebook inside the container:jupyter notebook --ip 0.0.0.0 --port 8888 --allow-rootMove the dataset to the /data directory inside the container. Download the notebook with the following command: wget --content-disposition https://api.ngc.nvidia.com/v2/resources/nvidia/med_3dunet/versions/1/zip -O med_3dunet_1.zipThen, upload the downloaded notebook into JupyterLab and run the cells of the notebook to preprocess the dataset and train, benchmark, and test the model.By running the cells of this Jupyter notebook, you can first check the downloaded dataset and see the brain tumor images. After that, see the data preprocessing command and prepare the data for training. The next step is training the model and using the checkpoints of the training process for the predicting step. Finally, check the output of the predict function visually.To check the dataset, you can use nibabel, which is a package that provides read/write access to some common medical and neuroimaging file formats.By running the next three cells, you can install nibabel using pip install, choose an image from the dataset, and plot the chosen third image from the dataset using matplotlib. You can check other dataset images by changing the image address in the code.The result is something like Figure 4.The dataset/preprocess_data.py script converts the raw data into the TFRecord format used for training and evaluation. This dataset, from the 2019 BraTS challenge, contains over 3 TB multiinstitutional, routine, clinically acquired, preoperative, multimodal, MRI scans of glioblastoma (GBM/HGG) and lower-grade glioma (LGG), with the pathologically confirmed diagnosis. When available, overall survival (OS) data for the patient is also included. This data is structured in training, validation, and testing datasets.The format of images is nii.gz. NIfTI is a type of file format for neuroimaging. You can preprocess the downloaded dataset by running the following command:python dataset/preprocess_data.py -i /data/MICCAI_BraTS_2019_Data_Training -o /data/preprocessed -vThe final format of the processed images is tfrecord. To help you read data efficiently, serialize your data and store it in a set of files (~100 to 200 MB each) that can each be read linearly. This is especially true if the data is being streamed over a network. It can also be useful for caching any data preprocessing. The TFRecord format is a simple format for storing a sequence of binary records, which speeds up the data loading process considerably.After the Docker container is launched, you can start the training of a single fold (fold 0) with the default hyperparameters (for example, {1 to 8} GPUs {TF-AMP/FP32/TF32}):Bash examples/unet3d_train_single{_TF-AMP}.sh <number/of/gpus> <path/to/dataset> <path/to/checkpoint> <batch/size>For example, to run with 32-bit precision (FP32 or TF32) with batch size 2 on one GPU, run the following command:bash examples/unet3d_train_single.sh 1 /data/preprocessed /results 2To train a single fold with mixed precision (TF-AMP) with on eight GPUs and batch size 2 per GPU, run the following command:bash examples/unet3d_train_single_TF-AMP.sh 8 /data/preprocessed /results 2The training performance can be evaluated by running benchmarking scripts:bash examples/unet3d_{train,infer}_benchmark{_TF-AMP}.sh <number/of/gpus/for/training> <path/to/dataset> <path/to/checkpoint> <batch/size>This script makes the model run and reports the performance. For example, to benchmark training with TF-AMP with batch size 2 on four GPUs, run the following command:bash examples/unet3d_train_benchmark_TF-AMP.sh 4 /data/preprocessed /results 2You can use the test dataset and predict as exec-mode to test the model. The result is saved in the model_dir directory and data_dir is the path to the dataset:python main.py --model_dir /results --exec_mode predict --data_dir /data/preprocessed_testIn the following code example, you plot one of the chosen results from the \\results folder:For those of you looking to explore advanced features built into this notebook, you can see the full list of available options for main.py using -h or --help. By running the next cell, you can see how to change execute mode and other parameters of this script. You can perform model training, predicting, evaluating, and inferencing using customized hyperparameters using this script.python main.py --helpThe main.py parameters can be changed to perform different tasks, including training, evaluation, and prediction.You can also train the model using default hyperparameters. By running the python main.py --help command, you can see the list of arguments that you can change, including training hyperparameters. For example, in training mode, you can change the learning rate from the default 0.0002 to 0.001 and the training steps from 16000 to 1000 using the following command:python main.py --model_dir /results --exec_mode train --data_dir /data/preprocessed_test --learning_rate 0.001 --max_steps 1000You can run other execution modes available in main.py. For example, in this post, we used the prediction execution mode of python.py by running the following command:python main.py --model_dir /results --exec_mode predict --data_dir /data/preprocessed_testIn this post, we showed how you can get started with a medical imaging model using a simple Jupyter notebook from the NGC catalog. As you make the transition from this Jupyter notebook to building your own medical imaging workflows, consider using NVIDIA Clara Train. Clara Train includes AI-Assisted Annotation APIs and an Annotation server that can be seamlessly integrated into any medical viewer, making it AI-capable. The training framework includes decentralized learning techniques, such federated learning and transfer learning for your AI workflows.To learn how to use these resources and kickstart your AI journey, register for the upcoming webinar with live Q&A, NVIDIA NGC Jupyter Notebook Day: Medical Imaging Segmentation.Have a story to share? Submit an idea.Get the developer news feed straight to your inbox.\n"
     ]
    }
   ],
   "source": [
    "!head -1 ../../../../dataset/EN/extractedNVblogs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-humor",
   "metadata": {},
   "source": [
    "## Back To Start Menu\n",
    "[start menu](../../../../Start_Here.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-strand",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Licensing\n",
    "\n",
    "This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
