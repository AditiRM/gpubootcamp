{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "egyptian-reply",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "# (option) : get your own data via scrap the website you have permisson to use\n",
    "\n",
    "note1 : strongly recommand to consult with your own legal department for compliance before proceeding this step on websites/webpages you have permission to\n",
    "\n",
    "note2 : modification needed when applying to different website/webpages\n",
    "\n",
    "note3 : use at your own risk !\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "- **The goal of this lab is to demonstrate that there are ways to obtain your own data via webscrapping sites which you have permission to**\n",
    "    - Provide a list of urls in a text file via collecting webpages links\n",
    "    \n",
    "    the base url used to crawl links from is  https://developer.nvidia.com/blog/\n",
    "    \n",
    "    - scrap the webpage ( in html format ) using [scrapy](https://docs.scrapy.org/en/latest/index.html) and obtain desired text, concatenate text per webpage into one raw text file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-delight",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install html5lib\n",
    "!pip install PyPDF2\n",
    "!pip install selenium\n",
    "!pip install Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-ultimate",
   "metadata": {},
   "source": [
    "## crawl NVblog landing page and obtain links to the individual blogs\n",
    "source github repo : https://github.com/x4nth055/pythoncode-tutorials/tree/master/web-scraping/link-extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "general-reaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-09-13 06:32:05--  https://raw.githubusercontent.com/x4nth055/pythoncode-tutorials/master/web-scraping/link-extractor/link_extractor.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3485 (3.4K) [text/plain]\n",
      "Saving to: ‘link_extractor.py’\n",
      "\n",
      "link_extractor.py   100%[===================>]   3.40K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-09-13 06:32:06 (74.1 MB/s) - ‘link_extractor.py’ saved [3485/3485]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/x4nth055/pythoncode-tutorials/master/web-scraping/link-extractor/link_extractor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sufficient-cement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-09-13 06:32:08--  https://raw.githubusercontent.com/x4nth055/pythoncode-tutorials/master/web-scraping/link-extractor/link_extractor_js.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3727 (3.6K) [text/plain]\n",
      "Saving to: ‘link_extractor_js.py’\n",
      "\n",
      "link_extractor_js.p 100%[===================>]   3.64K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-09-13 06:32:08 (71.3 MB/s) - ‘link_extractor_js.py’ saved [3727/3727]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/x4nth055/pythoncode-tutorials/master/web-scraping/link-extractor/link_extractor_js.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests bs4 colorama requests-html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "spoken-scottish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[*] Crawling: https://blogs.nvidia.com/blog/category/deep-learning/\u001b[39m\n",
      "[W:pyppeteer.chromium_downloader] Starting Chromium download. Download may take a few minutes.\n",
      "100%|████████████████████████████████████████| 109M/109M [00:01<00:00, 55.0Mb/s]\n",
      "[W:pyppeteer.chromium_downloader] Chromium download done.\n",
      "[W:pyppeteer.chromium_downloader] chromium extracted to: /root/.local/share/pyppeteer/local-chromium/588429\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/category/deep-learning/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com\u001b[39m\n",
      "\u001b[90m[!] External link: https://twitter.com/nvidia\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.facebook.com/NVIDIA\u001b[39m\n",
      "\u001b[90m[!] External link: https://news.ycombinator.com/submit\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.instagram.com/nvidia/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.linkedin.com/company/nvidia/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.reddit.com/r/nvidia/\u001b[39m\n",
      "\u001b[90m[!] External link: https://feeds.feedburner.com/nvidiablog\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.twitch.tv/nvidia\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.youtube.com/nvidia\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/privacy-commenting-policy/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/about-nvidia/legal-info/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/contact/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/category/enterprise/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/category/auto/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/category/gaming/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/category/pro-graphics/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/category/autonomous-machines/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/tag/medical-research-and-healthcare/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/tag/inception/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/ai-podcast/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/2021/08/16/what-is-a-machine-learning-model/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/2021/08/13/nvidia-brings-metaverse-momentum-research-breakthroughs-and-new-pro-gpu-to-siggraph/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/2021/08/11/omniverse-making-of-gtc/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/2021/08/11/lending-a-helping-hand-jules-anh-tuan-nguyen-on-building-a-neuroprosthetic/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/2021/08/12/geforce-now-thursday-august-12/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/2021/08/11/cloudxr-google-cloud-demo/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/2021/08/10/omniverse-connector-extension-plugin-siggraph/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/about-nvidia/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/about-nvidia/ai-computing/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/technologies/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/research/\u001b[39m\n",
      "\u001b[90m[!] External link: https://investor.nvidia.com/home/default.aspx\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/csr/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/foundation/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/forums/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/about-nvidia/careers/\u001b[39m\n",
      "\u001b[90m[!] External link: https://developer.nvidia.com/\u001b[39m\n",
      "\u001b[90m[!] External link: https://developer.nvidia.com/developer-program\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/about-nvidia/partners/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/deep-learning-ai/startups/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/deep-learning-ai/startups/venture-capital/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/about-nvidia/gpu-ventures/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/training/\u001b[39m\n",
      "\u001b[90m[!] External link: https://academy.mellanox.com/en/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/ai-data-science/professional-services/\u001b[39m\n",
      "\u001b[90m[!] External link: https://nvidianews.nvidia.com/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/about-nvidia/webinar-portal/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/preferences/email-signup/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/events/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/gtc/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/on-demand/\u001b[39m\n",
      "\u001b[90m[!] External link: mailto://blogideas@nvidia.com\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/contact/social/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/location-selector/\u001b[39m\n",
      "\u001b[90m[!] External link: https://twitter.com/intent/tweet\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.facebook.com/dialog/share\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/wp-admin/admin-ajax.php\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.linkedin.com/shareArticle\u001b[39m\n",
      "\u001b[33m[*] Crawling: https://blogs.nvidia.com/blog/category/pro-graphics/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/2021/08/11/applications-open-graduate-fellowship-awards/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/2021/08/10/siggraph-real-time-live-demo/\u001b[39m\n",
      "\u001b[33m[*] Crawling: https://blogs.nvidia.com/blog/2021/08/10/siggraph-real-time-live-demo/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/author/ishasalian/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.facebook.com/sharer/sharer.php\u001b[39m\n",
      "\u001b[90m[!] External link: http://news.ycombinator.com/submitlink\u001b[39m\n",
      "\u001b[90m[!] External link: https://s2021.siggraph.org/program/real-time-live/\u001b[39m\n",
      "\u001b[90m[!] External link: https://s2021.siggraph.org/presentation/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/design-visualization/rtx-professional-laptops/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/design-visualization/rtx-a6000/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/2021/06/24/vid2vid-cameo-ai-research-video-conferencing/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/omniverse/apps/audio2face/\u001b[39m\n",
      "\u001b[90m[!] External link: https://github.com/NVlabs/stylegan\u001b[39m\n",
      "\u001b[90m[!] External link: https://openreview.net/pdf\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.nvidia.com/en-us/events/siggraph/\u001b[39m\n",
      "\u001b[90m[!] External link: https://www.youtube.com/watch\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/category/nvidia-research/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/tag/artificial-intelligence/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/tag/events/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/tag/nvidia-research/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/tag/nvidia-rtx/\u001b[39m\n",
      "\u001b[32m[*] Internal link: https://blogs.nvidia.com/blog/tag/siggraph/\u001b[39m\n",
      "[+] Total Internal links: 30\n",
      "[+] Total External links: 53\n",
      "[+] Total URLs: 83\n",
      "[+] Total crawled URLs: 2\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python link_extractor_js.py https://blogs.nvidia.com/blog/category/deep-learning/ -m 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "appreciated-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as request\n",
    "import re\n",
    "import scrapy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-indonesia",
   "metadata": {},
   "source": [
    "## fetch the urls of interest and convert to html files \n",
    "### noted the url in NVdevblog_url.txt has been pre-filtered to ensure only NVblog pages are scrapped !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "gentle-estate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('nvidia_and_mozilla_release_common_voice_dataset_surpassing_13000_hours_for_the_first_time.html',\n",
       " 'https://developer.nvidia.com/blog/nvidia-and-mozilla-release-common-voice-dataset-surpassing-13000-hours-for-the-first-time/')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import os, sys\n",
    "# create folder to hold the scrapped html pages\n",
    "os.makedirs('./htmls/', exist_ok=True)\n",
    "f=open('NVdevblog_urls.txt','r')\n",
    "lines=f.readlines()\n",
    "rn=random.randint(0,len(lines)-1)\n",
    "url=str(lines[rn]).strip()\n",
    "name=url.split(':')[1][2:].split('/')[-2].replace('-','_')+'.html'\n",
    "name=str(name)\n",
    "name, url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-upset",
   "metadata": {},
   "source": [
    "## fetch given url and save as .html file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "indie-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import scrapy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "meaning-northwest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./htmls/response_0.html\n",
      "./htmls/response_1.html\n",
      "./htmls/response_2.html\n",
      "./htmls/response_3.html\n",
      "./htmls/response_4.html\n",
      "./htmls/response_5.html\n",
      "./htmls/response_6.html\n",
      "./htmls/response_7.html\n",
      "./htmls/response_8.html\n",
      "./htmls/response_9.html\n",
      "./htmls/response_10.html\n",
      "./htmls/response_11.html\n",
      "./htmls/response_12.html\n",
      "./htmls/response_13.html\n",
      "./htmls/response_14.html\n",
      "./htmls/response_15.html\n",
      "./htmls/response_16.html\n",
      "./htmls/response_17.html\n",
      "./htmls/response_18.html\n",
      "./htmls/response_19.html\n",
      "./htmls/response_20.html\n",
      "./htmls/response_21.html\n",
      "./htmls/response_22.html\n",
      "./htmls/response_23.html\n",
      "./htmls/response_24.html\n",
      "./htmls/response_25.html\n",
      "./htmls/response_26.html\n",
      "./htmls/response_27.html\n",
      "./htmls/response_28.html\n",
      "./htmls/response_29.html\n",
      "./htmls/response_30.html\n",
      "./htmls/response_31.html\n",
      "./htmls/response_32.html\n",
      "./htmls/response_33.html\n",
      "./htmls/response_34.html\n",
      "./htmls/response_35.html\n",
      "./htmls/response_36.html\n",
      "./htmls/response_37.html\n",
      "./htmls/response_38.html\n",
      "./htmls/response_39.html\n",
      "./htmls/response_40.html\n",
      "./htmls/response_41.html\n",
      "./htmls/response_42.html\n",
      "./htmls/response_43.html\n",
      "./htmls/response_44.html\n",
      "./htmls/response_45.html\n",
      "./htmls/response_46.html\n",
      "./htmls/response_47.html\n",
      "./htmls/response_48.html\n",
      "./htmls/response_49.html\n",
      "./htmls/response_50.html\n",
      "./htmls/response_51.html\n",
      "./htmls/response_52.html\n",
      "./htmls/response_53.html\n",
      "./htmls/response_54.html\n",
      "./htmls/response_55.html\n",
      "./htmls/response_56.html\n",
      "./htmls/response_57.html\n",
      "./htmls/response_58.html\n",
      "./htmls/response_59.html\n",
      "./htmls/response_60.html\n",
      "./htmls/response_61.html\n",
      "./htmls/response_62.html\n",
      "./htmls/response_63.html\n",
      "./htmls/response_64.html\n",
      "./htmls/response_65.html\n",
      "./htmls/response_66.html\n",
      "./htmls/response_67.html\n",
      "./htmls/response_68.html\n",
      "./htmls/response_69.html\n",
      "./htmls/response_70.html\n"
     ]
    }
   ],
   "source": [
    "!bash fetchURLs_and_write2html.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "hindu-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "import html5lib\n",
    "import codecs\n",
    "import os,sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "collectible-portal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish processing htmls files and convert them to \n"
     ]
    }
   ],
   "source": [
    "# read the html into python\n",
    "\n",
    "def covert2txt(html_f ,f_out):\n",
    "    file = codecs.open(html_f, \"r\", \"utf-8\")\n",
    "    html_doc=file.read()\n",
    "    soup = BeautifulSoup(html_doc)\n",
    "    sent_cnt=0\n",
    "    for node in soup.findAll('p'):\n",
    "        #print(type(node.text), node.text)\n",
    "        if node.text not in ['/n','','\\t',' ','\\n\\r'] : \n",
    "            sent_cnt+=1\n",
    "            f_out.write(node.text)            \n",
    "    f_out.write('\\n')      \n",
    "html_dir='./htmls/'\n",
    "htmls=os.listdir('./htmls')\n",
    "f_out=open('extractedNVblogs.txt' , 'a')\n",
    "for html in htmls:\n",
    "    outtxt=html.split('.')[0]   \n",
    "    covert2txt(html_dir+html ,f_out)\n",
    "f_out.close()\n",
    "print(\"finish processing htmls files and convert them to raw txt file\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-morocco",
   "metadata": {},
   "source": [
    "---\n",
    "# move the extractedNVblogs.txt to the dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "apparent-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv extractedNVblogs.txt ../../../../dataset/EN/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-sterling",
   "metadata": {},
   "source": [
    "---\n",
    "## clearn up the htmls folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "arabic-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -fr htmls*\n",
    "!rm link_extractor.py\n",
    "!rm link_extractor_js.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-airfare",
   "metadata": {},
   "source": [
    "--- \n",
    "# doucle check the raw text file looks oki "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "alien-moment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a high chance that you have asked your smart speaker a question like, “How tall is Mount Everest?” If you did, it probably said, “Mount Everest is 29,032 feet above sea level.” Have you ever wondered how it found an answer for you?Question answering (QA) is loosely defined as a system consisting of information retrieval (IR) and natural language processing (NLP), which is concerned with answering questions posed by humans in a natural language. If you are not familiar with information retrieval, it is a technique to obtain relevant information to a query, from a pool of resources, webpages, or documents in the database, for example. The easiest way to understand the concept is the search engine that you use daily. You then need an NLP system to find an answer within the IR system that is relevant to the query. Although I just listed what you need for building a QA system, it is not a trivial task to build IR and NLP from scratch. Here’s how NVIDIA Riva makes it easy to develop a QA system.NVIDIA Riva is a GPU-accelerated SDK for building multimodal conversational AI services that use an end-to-end deep learning pipeline. The Riva framework includes optimized services for speech, vision, and natural language understanding (NLU) tasks. In addition to providing several pretrained models for the entire pipeline of your conversational AI service, Riva is also architected for deployment at scale. In this post, I look closely into the QA function of Riva and how you can create your own QA application with it.To understand how the Riva QA function works, start with Bidirectional Encoder Representations from Transformers (BERT). It’s a transformer-based, NLP, pretraining method developed by Google in 2018, and it completely changed the field of NLP. BERT understands the contextual representation of a given word in a text. It is pretrained on a large corpus of data, including Wikipedia. With the pretrained BERT, a strong NLP engine, you can further fine-tune it to perform QA with many question-answer pairs like those in the Stanford Question Answering Dataset (SQuAD). The model can now find an answer for a question in natural language from a given context: sentences or paragraphs. Figure 1 shows an example of QA, where it highlights the word “gravity” as an answer to the query, “What causes precipitation to fall?”. In this example, the paragraph is the context and the successfully fine-tuned QA model returns the word “gravity” as an answer.Teams of engineers and researchers at NVIDIA deliver a quality QA function that you can use right out-of-the-box with Riva. The Riva NLP service provides a set of high-level API actions that include QA, NaturalQuery. The Wikipedia API action allows you to fetch articles posted on Wikipedia, an online encyclopedia, with a query in natural language. That’s the information retrieval system that I discussed earlier. Combining the Wikipedia API action and Riva QA function, you can create a simple QA system with a few lines of Python code. Start by installing the Wikipedia API for Python. Next, import the Riva NLP service API and gRPC, the underlying communication framework for Riva.Now, create an input query. Use the Wikipedia API action to fetch the relevant articles and define the number of them to fetch, defined as max_articles_combine. Ask a question, “What is speech recognition?” You then print out the titles of the articles returned from the search. Finally, you add the summaries of each article into a variable: combined_summary.Next, open a gRPC channel that points to the location where the Riva server is running. Because you are running the Riva server locally, it is ‘localhost:50051‘. Then, instantiate NaturalQueryRequest, and send a request to the Riva server, passing both the query and the context. Finally, print the response, returned from the Riva server.With Riva QA and the Wikipedia API action, you just created a simple QA application. If there’s an article in Wikipedia that is relevant to your query, you can theoretically find answers. Imagine that you have a database full of articles relevant to your domain, company, industry, or anything of interest. You can create a QA service that can find answers to the questions specific to your field of interest. Obviously, you would need an IR system that would fetch relevant articles from your database, like the Wikipedia API action used in this post. When you have the IR system in your pipeline, Riva can help you find an answer for you. We look forward to the cool applications that you’ll create with Riva. .Have a story to share? Submit an idea.Get the developer news feed straight to your inbox.\n"
     ]
    }
   ],
   "source": [
    "!head -1 ../../../../dataset/EN/extractedNVblogs.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
