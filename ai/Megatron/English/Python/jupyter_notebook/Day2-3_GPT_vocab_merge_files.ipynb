{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "valid-smoke",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "# 3_About GPT vocab and merge files\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "- **The goal of this lab is to:**\n",
    "    - the difference between BPE and GPTBPE Tokenizer\n",
    "    - load and verify GPTBPE Tokenizer can do tokenization as expected \n",
    "\n",
    "\n",
    "Download the GPT vocab and merge files \n",
    "\n",
    "Download vocab file [English_vocab](https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json)\n",
    "\n",
    "Download merge file [English_merge](https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-crash",
   "metadata": {},
   "source": [
    "#### let's review the source code of [gpt2 tokenizer](https://huggingface.co/transformers/_modules/transformers/tokenization_gpt2.html)\n",
    "\n",
    "Construct a GPT-2 tokenizer. Based on byte-level Byte-Pair-Encoding.\n",
    "\n",
    "    This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n",
    "    be encoded differently whether it is at the beginning of the sentence (without space) or not:\n",
    "\n",
    "    ::\n",
    "\n",
    "         from transformers import GPT2Tokenizer\n",
    "         tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        \n",
    "         tokenizer(\" Hello world\")['input_ids']\n",
    "        [18435, 995]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-vehicle",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tokenizers  transformers ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-substance",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n",
    "!wget https://huggingface.co/openai-gpt/resolve/main/vocab.json\n",
    "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n",
    "!wget https://huggingface.co/openai-gpt/resolve/main/merges.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "copyrighted-present",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-specific",
   "metadata": {},
   "source": [
    "## examine the vocab and merge files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "chemical-ebony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noted that the Ġ = space +256 to form that control letter\n",
      "['Ġdegrees', 'Donald', 'Ġcentre', 'Ġsharing', 'Ġwinter', 'ĠCO', 'Che', 'ĠÎ', 'MP', 'Ġunw', 'Ġfewer', 'ĠMir', 'Ġsomewhere', 'ĠKey', 'Ġattacked', 'ĠKir', 'Ġdomain', 'Ġstronger', 'Ġ99', 'Ġpenalty']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "with open('gpt2-vocab.json') as ip_file:\n",
    "    o = json.load(ip_file)\n",
    "    take=20\n",
    "    rn=random.randint(0,len(o)-1)\n",
    "    print(\"noted that the Ġ = space + 256 to form that control letter\")\n",
    "    print(list(o.keys())[rn:rn+take])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "mediterranean-outreach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "om inated\n",
      "Ġreg ress\n",
      "ĠColl ider\n",
      "Ġinform ants\n",
      "Ġg azed\n"
     ]
    }
   ],
   "source": [
    "!tail -n 5 gpt2-merges.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-albuquerque",
   "metadata": {},
   "source": [
    "## sanity check load from transformer GPT2Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "classical-stretch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " notice the **SPACE** in front of ** Hello world** \n",
      "\n",
      " Hello world\n",
      "tokens: ['ĠHello', 'Ġworld']\n",
      "ids: [18435, 995]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "print('\\n notice the **SPACE** in front of ** Hello world** \\n')\n",
    "sample_text=\" Hello world\"\n",
    "print(sample_text)\n",
    "out=tokenizer.tokenize(sample_text)\n",
    "print(\"tokens:\",out)\n",
    "ids=tokenizer(sample_text)['input_ids']\n",
    "print(\"ids:\",ids)\n",
    "## expected output :\n",
    "## [18435, 995]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cordless-science",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens:  ['ĠHello', 'Ġworld']\n",
      "ids: [18435, 995]\n",
      "------------------------------\n",
      "\n",
      "notice the difference when using BPE as tokenizer instead of GPT2BPE tokenizer\n",
      "tokens:  ['H', 'ellow', 'orld']\n",
      "ids: [39, 5037, 1764]\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.models import BPE\n",
    "import json\n",
    "\n",
    "\n",
    "def load_tokenizer(vocab_file,merge_file, gpt2):\n",
    "    tokenizer = Tokenizer(BPE())\n",
    "    tokenizer.model = BPE.from_file(vocab_file, merge_file)\n",
    "    with open(vocab_file, 'r') as f2:\n",
    "        vocab = json.loads(f2.read())  \n",
    "    if gpt2:\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "        tokenizer.decoder = ByteLevelDecoder()\n",
    "    return tokenizer , vocab\n",
    "vocab_file='./Megatron-LM/gpt2-vocab.json'\n",
    "merge_file='./Megatron-LM/gpt2-merges.txt'\n",
    "tokenizers_gpt,_=load_tokenizer(vocab_file,merge_file,True)\n",
    "sample_text=' Hello world' \n",
    "output=tokenizers_gpt.encode(sample_text)\n",
    "ids=output.ids\n",
    "tokens=output.tokens\n",
    "#print(tokens ,'\\n')\n",
    "print(\"tokens: \",tokens)\n",
    "print(\"ids:\",ids)\n",
    "\n",
    "tokenizers_bpe,_=load_tokenizer(vocab_file,merge_file, False)\n",
    "sample_text=' Hello world'\n",
    "output=tokenizers_bpe.encode(sample_text)\n",
    "ids=output.ids\n",
    "tokens=output.tokens\n",
    "\n",
    "print(\"---\"*10)\n",
    "print('\\nnotice the difference when using BPE as tokenizer instead of GPT2BPE tokenizer')\n",
    "print(\"tokens: \",tokens)\n",
    "print(\"ids:\",ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-latest",
   "metadata": {},
   "source": [
    "---\n",
    "## Up Next : \n",
    "\n",
    "[Jsonfy and convert to mmap ](./Day2-4_jsonfy_and_process2mmap.ipynb)\n",
    "\n",
    "## Back To Start Menu\n",
    "[start menu](../Start_Here.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-ecology",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "\n",
    "## Licensing \n",
    "\n",
    "This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
