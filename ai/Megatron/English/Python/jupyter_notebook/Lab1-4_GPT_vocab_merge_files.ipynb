{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "noticed-neighborhood",
   "metadata": {},
   "source": [
    "## GPT Tokenizer files\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "The goal of this lab is to examine the difference between BPE and GPTBPE Tokenizer.\n",
    "\n",
    "Later on, we will use the observations from this notebook to train a GPT Tokenizer with our own raw text data.\n",
    "\n",
    "We will load and verify GPTBPE Tokenizer and make sure the output tokens and token ids are as expected. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-architecture",
   "metadata": {},
   "source": [
    "Let's review the source code of [gpt2 tokenizer](https://huggingface.co/transformers/_modules/transformers/tokenization_gpt2.html)\n",
    "\n",
    "    This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n",
    "    be encoded differently whether it is at the beginning of the sentence (without space) or not:\n",
    "\n",
    "    \n",
    "\n",
    "         from transformers import GPT2Tokenizer\n",
    "         tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        \n",
    "         tokenizer(\" Hello world\")['input_ids']\n",
    "        [18435, 995]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-protocol",
   "metadata": {},
   "source": [
    "Install necessary python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-owner",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tokenizers transformers ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-glenn",
   "metadata": {},
   "source": [
    "Next, we proceed to fetch pretrained GPT Tokenizer files, namely the vocab and merge files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n",
    "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-plenty",
   "metadata": {},
   "source": [
    "Examine the vocab and merge files, noted the presence of Ġ character.\n",
    "Ġ = space + 256 , this character is used as a control letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-carbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "with open('gpt2-vocab.json') as ip_file:\n",
    "    o = json.load(ip_file)\n",
    "    take=20\n",
    "    rn=random.randint(0,len(o)-1)\n",
    "    print(\"noted that the Ġ = space + 256 is the control letter\")\n",
    "    print(list(o.keys())[rn:rn+take])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -n 5 gpt2-merges.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-baker",
   "metadata": {},
   "source": [
    "The following code block will load GPT2Tokenizer from HuggingFace transformer library, we verify the following :\n",
    "\n",
    "            from transformers import GPT2Tokenizer\n",
    "            tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        \n",
    "            tokenizer(\" Hello world\")['input_ids']\n",
    "            expected token ids for \" Hello world\" is [18435, 995]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "print('\\n notice the **SPACE** in front of ** Hello world** \\n')\n",
    "sample_text=\" Hello world\"\n",
    "print(sample_text)\n",
    "out=tokenizer.tokenize(sample_text)\n",
    "print(\"tokens:\",out)\n",
    "ids=tokenizer(sample_text)['input_ids']\n",
    "print(\"ids:\",ids)\n",
    "## expected output :\n",
    "## [18435, 995]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-harris",
   "metadata": {},
   "source": [
    "Below is the expected outputs :\n",
    "    \n",
    "         Hello world\n",
    "        tokens: ['ĠHello', 'Ġworld']\n",
    "        ids: [18435, 995]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-brick",
   "metadata": {},
   "source": [
    "Next code block will load tokenizer library from huggingFace, we will observe the difference when setting `use_gpt` to True or False. \n",
    "\n",
    "Setting `use_gpt` to True will evoke the following : \n",
    "\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "        tokenizer.decoder = ByteLevelDecoder()\n",
    "        \n",
    "This is the expected tokenizer behavior for GPT models, namely GPTBPE Tokenizer, this GPTBPE tokenizer will load the vocab.json and merges.txt files and tokenize as expected. Whereas setting `use_gpt` to False, will result in a normal BPE Tokenizer, the tokenization will behave differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-strike",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.models import BPE\n",
    "import json\n",
    "\n",
    "def load_tokenizer(vocab_file,merge_file, use_gpt):\n",
    "    tokenizer = Tokenizer(BPE())\n",
    "    tokenizer.model = BPE.from_file(vocab_file, merge_file)\n",
    "    with open(vocab_file, 'r') as f2:\n",
    "        vocab = json.loads(f2.read())  \n",
    "    if use_gpt:\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "        tokenizer.decoder = ByteLevelDecoder()\n",
    "    return tokenizer , vocab\n",
    "vocab_file='./gpt2-vocab.json'\n",
    "merge_file='./gpt2-merges.txt'\n",
    "tokenizers_gpt,_=load_tokenizer(vocab_file,merge_file,True)\n",
    "sample_text=' Hello world' \n",
    "output=tokenizers_gpt.encode(sample_text)\n",
    "ids=output.ids\n",
    "tokens=output.tokens\n",
    "#print(tokens ,'\\n')\n",
    "print(\"tokens: \",tokens)\n",
    "print(\"ids:\",ids)\n",
    "\n",
    "tokenizers_bpe,_=load_tokenizer(vocab_file,merge_file, False)\n",
    "sample_text=' Hello world'\n",
    "output=tokenizers_bpe.encode(sample_text)\n",
    "ids=output.ids\n",
    "tokens=output.tokens\n",
    "print(\"---\"*10)\n",
    "print('\\nnotice the difference when using BPE as tokenizer instead of GPT2BPE tokenizer')\n",
    "print(\"tokens: \",tokens)\n",
    "print(\"ids:\",ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-prize",
   "metadata": {},
   "source": [
    "Below is the expected outputs :\n",
    "\n",
    "        tokens:  ['ĠHello', 'Ġworld']\n",
    "        ids: [18435, 995]\n",
    "        ------------------------------\n",
    "\n",
    "        notice the difference when using BPE as tokenizer instead of GPT2BPE tokenizer\n",
    "        tokens:  ['H', 'ellow', 'orld']\n",
    "        ids: [39, 5037, 1764]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-conflict",
   "metadata": {},
   "source": [
    "What did we observed ? \n",
    "\n",
    "Setting `use_gpt` to True will give us the expected behavor of GPTBPE tokenization. \n",
    "\n",
    "It will ensure the presence of Ġ : \n",
    "\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "    tokenizer.decoder = ByteLevelDecoder()\n",
    "\n",
    "\n",
    "Therefore, we will enforce having :\n",
    "\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "    tokenizer.decoder = ByteLevelDecoder()\n",
    "When training our own GPTBPETokenizer with our own raw text data.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-lancaster",
   "metadata": {},
   "source": [
    "We will now move the gpt-vocab.json and gpt2-merges.txt to the correct data folder as a preparation for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv gpt2-vocab.json ../dataset/EN/50k/\n",
    "!mv gpt2-merges.txt ../dataset/EN/50k/\n",
    "!ls ../dataset/EN/50k/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-horse",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Links and Resources\n",
    "Don't forget to check out additional resources such as [HuggingFace Tokenizer Documentation](https://huggingface.co/docs/tokenizers/python/latest/quicktour.html) and [Train GPT-2 in your own langauge](https://towardsdatascience.com/train-gpt-2-in-your-own-language-fc6ad4d60171).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-dressing",
   "metadata": {},
   "source": [
    "-----\n",
    "## <p style=\"text-align:center;border:3px; padding: 1em\"> <a href=../Start_Here.ipynb>HOME</a>&nbsp; &nbsp; &nbsp; <a href=./Lab1-5_jsonfy_and_process2mmap.ipynb>NEXT</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-greece",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "\n",
    "## Licensing \n",
    "\n",
    "This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
