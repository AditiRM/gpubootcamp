{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "boxed-privilege",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "# 5 Monitor GPT training performance with varying config\n",
    "---\n",
    "\n",
    "## **Challenge ** - Go big or go home !\n",
    "- prerequisites : \n",
    "    - use your current given # of gpus \n",
    "    - do NOT changing the following parameters **--train-samples 100 **\n",
    "    - you cannot go OOM \n",
    "    - you must sustain >60% GPUs utilization in the **training** phase \n",
    "    - training run must be finished and checkpoint must be saved successfully\n",
    "\n",
    "\n",
    "- task : \n",
    "        given the above constraints, train as BIG GPT model as possible\n",
    "\n",
    "\n",
    "\n",
    "- winning criteria : the biggest model wins given the above constraints(=prerequisites).\n",
    "\n",
    "    <a href=\"./Day3-5_run_Megatron_with_varying_config.ipynb#Rerun_Cell\">Jump to ReRun Cell</a> \n",
    "\n",
    "```\n",
    "                                #### the follow params are allowed to change \n",
    "                                WORLD_SIZE=8 # <--- remember to change the number of GPUs you actually have in your system\n",
    "                                GPUS_PER_NODE=8 # <--- remember to change the number of GPUs you actually have in your system\n",
    "\n",
    "                                TENSOR_MP_SIZE=8\n",
    "                                PIPELINE_MP_SIZE=1\n",
    "                                LYS=32\n",
    "                                HIDDEN_SZ=2048\n",
    "                                NUM_ATTN_HEADS=32\n",
    "                                MICRO_BZ=\n",
    "                                GLOBAL_BZ=\n",
    "                                SEQ_LEN=\n",
    "                                MAX_POS_EM=\n",
    "                                #### ---------------------------#### \n",
    "``` \n",
    "                                ----------------------------For your reference --------------------------\n",
    "<center><img src=\"./Megatron-LM/pics/GPT3_all.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-holiday",
   "metadata": {},
   "source": [
    "<a id=\"Rerun_Cell\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "opening-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -fr ./Megatron-LM/sv_ckpt/* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "future-explorer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./Megatron-LM/profile_SVGPT_BIG.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./Megatron-LM/profile_SVGPT_BIG.sh\n",
    "# Copyright (c) 2020 NVIDIA Corporation.  All rights reserved.\n",
    "MASTER_ADDR=localhost\n",
    "MASTER_PORT=6000\n",
    "NNODES=1 #<-- currently we are using 1 node multigpus\n",
    "NODE_RANK=0\n",
    "\n",
    "### modify this section to point the file to its own path \n",
    "CHECKPOINT_PATH='./Megatron-LM/sv_ckpt/'\n",
    "DATA_PATH='../dataset/SV/webnyheter2013_text_document'\n",
    "VOCAB_FILE='../dataset/SV/32k/vocab.json'\n",
    "MERGE_FILE='../dataset/SV/32k/merges.txt'\n",
    "PROFILE_OUTPUT_PATH='/home/zcharpy/profiles/DLprof/2ndrun/nsys_improved' # modify this to your own profile path\n",
    "\n",
    "#### [TODO]--------------- params in the following block are allowed to change -----------#### \n",
    "WORLD_SIZE=8 # <--- remember to change the number of GPUs you actually have in your system\n",
    "GPUS_PER_NODE=8 # <--- remember to change the number of GPUs you actually have in your system\n",
    "\n",
    "TENSOR_MP_SIZE=8\n",
    "PIPELINE_MP_SIZE=1\n",
    "LAYERS=64\n",
    "HIDDEN_SZ=2048\n",
    "NUM_ATTN_HEADS=32\n",
    "MICRO_BZ=64\n",
    "GLOBAL_BZ=512\n",
    "SEQ_LEN=512\n",
    "MAX_POS_EM=512\n",
    "#### -------------------- end of blocks ------------------------#### \n",
    "\n",
    "export OMP_NUM_THREADS=1\n",
    "DISTRIBUTED_ARGS=\"--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port $MASTER_PORT\"\n",
    "\n",
    "## for nsys run\n",
    "#nsys profile --stats=false --force-overwrite=true --duration=300 --trace=cudnn,cuda,osrt,nvtx -o $PROFILE_OUTPUT_PATH \\\n",
    "python -m torch.distributed.launch $DISTRIBUTED_ARGS \\\n",
    "    ./Megatron-LM/Dlprof_pretrain_gpt.py \\\n",
    "       --tensor-model-parallel-size $TENSOR_MP_SIZE \\\n",
    "       --pipeline-model-parallel-size $PIPELINE_MP_SIZE \\\n",
    "       --num-layers $LAYERS \\\n",
    "       --hidden-size $HIDDEN_SZ \\\n",
    "       --num-attention-heads $NUM_ATTN_HEADS \\\n",
    "       --micro-batch-size $MICRO_BZ \\\n",
    "       --global-batch-size $GLOBAL_BZ \\\n",
    "       --seq-length $SEQ_LEN \\\n",
    "       --max-position-embeddings $MAX_POS_EM \\\n",
    "       --train-samples 100 \\\n",
    "       --save $CHECKPOINT_PATH \\\n",
    "       --load $CHECKPOINT_PATH \\\n",
    "       --data-path 1. $DATA_PATH \\\n",
    "       --vocab-file $VOCAB_FILE \\\n",
    "       --merge-file $MERGE_FILE \\\n",
    "       --data-impl mmap \\\n",
    "       --split 949,50,1 \\\n",
    "       --distributed-backend nccl \\\n",
    "       --lr 0.00015 \\\n",
    "       --lr-decay-style cosine \\\n",
    "       --min-lr 1.0e-5 \\\n",
    "       --weight-decay 1e-2 \\\n",
    "       --clip-grad 1.0 \\\n",
    "       --lr-warmup-fraction .01 \\\n",
    "       --checkpoint-activations \\\n",
    "       --log-interval 10 \\\n",
    "       --save-interval 100 \\\n",
    "       --eval-interval 200 \\\n",
    "       --eval-iters 10 \\\n",
    "       --fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-prerequisite",
   "metadata": {},
   "source": [
    "---\n",
    "## check how big is your model - \n",
    "I got 1 Billion :)  what about you ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "affecting-function",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3289513984\n"
     ]
    }
   ],
   "source": [
    "!bash params_cnt.sh $LAYERS $HIDDEN_SZ $NUM_ATTN_HEADS $SEQ_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-dominican",
   "metadata": {},
   "source": [
    "---\n",
    "#### you should see something similar to the following \n",
    "\n",
    "            training ...\n",
    "            time (ms) | model-and-optimizer-setup: 4013.85 | train/valid/test-data-iterators-setup: 2773.74\n",
    "            [after training is done] datetime: 2021-08-27 06:24:46 \n",
    "            ------------------------------------------------------------------------------------------------------------------\n",
    "             validation loss at the end of training for val data | lm loss value: 1.124495E+01 | lm loss PPL: 7.649290E+04 | \n",
    "            ------------------------------------------------------------------------------------------------------------------\n",
    "            Processing events...\n",
    "            Capturing symbol files...\n",
    "            Saving temporary \"/tmp/nsys-report-96a7-0101-ea4b-0ee5.qdstrm\" file to disk...\n",
    "            Creating final output files...\n",
    "\n",
    "            Processing [==============================================================100%]\n",
    "            Saved report file to \"/tmp/nsys-report-96a7-0101-ea4b-0ee5.qdrep\"\n",
    "            Report file moved to \"/home/zcharpy/profiles/DLprof/2ndrun/nsys_improved.qdrep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "acknowledged-brake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing NVTX monkey patches\n",
      "Initializing NVTX monkey patches\n",
      "Initializing NVTX monkey patches\n",
      "Initializing NVTX monkey patches\n",
      "Initializing NVTX monkey patches\n",
      "Initializing NVTX monkey patches\n",
      "Initializing NVTX monkey patches\n",
      "Initializing NVTX monkey patches\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "Done with NVTX monkey patching\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "Done with NVTX monkey patching\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "Done with NVTX monkey patching\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "Done with NVTX monkey patching\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "Done with NVTX monkey patching\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "Done with NVTX monkey patching\n",
      "Done with NVTX monkey patching\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "Done with NVTX monkey patching\n",
      "using world size: 8, data-parallel-size: 1, tensor-model-parallel size: 8, pipeline-model-parallel size: 1 \n",
      "using torch.float16 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  checkpoint_activations .......................... True\n",
      "  checkpoint_num_layers ........................... 1\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['1.', '../dataset/SV/webnyheter2013_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  encoder_seq_length .............................. 512\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 200\n",
      "  eval_iters ...................................... 10\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ True\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  global_batch_size ............................... 512\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 64\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./Megatron-LM/sv_ckpt/\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. None\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 512\n",
      "  merge_file ...................................... ../dataset/SV/32k/merges.txt\n",
      "  micro_batch_size ................................ 64\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 32\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 64\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float16\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./Megatron-LM/sv_ckpt/\n",
      "  save_interval ................................... 100\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 512\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 8\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... None\n",
      "  train_samples ................................... 100\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_ddp ................... False\n",
      "  use_cpu_initialization .......................... None\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... ../dataset/SV/32k/vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 8\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 8\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 32000) with 768 dummy tokens (new size: 32768)\n",
      "> initializing torch distributed ...\n",
      "> initializing tensor model parallel with size 8\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "> compiling dataset index builder ...\n",
      "make: Entering directory '/home/zcharpy/bootcamp/jupyter_notebook/Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/zcharpy/bootcamp/jupyter_notebook/Megatron-LM/megatron/data'\n",
      ">>> done with dataset index builder. Compilation time: 0.167 seconds\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/zcharpy/bootcamp/jupyter_notebook/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/zcharpy/bootcamp/jupyter_notebook/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/zcharpy/bootcamp/jupyter_notebook/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 18.065 seconds\n",
      "time to initialize megatron (seconds): 90.261\n",
      "[after megatron is initialized] datetime: 2021-08-30 08:59:22 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 412995584\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (4, 0): 412995584\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (6, 0): 412995584\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (7, 0): 412995584\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 412995584\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 412995584\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 412995584\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (5, 0): 412995584\n",
      "setting training iterations to 0\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./Megatron-LM/sv_ckpt/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 25.10\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2021-08-30 08:59:28 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      100\n",
      "    validation: 5120\n",
      "    test:       5120\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.004143 seconds\n",
      "    number of documents: 1249010\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 1185311) total of 1185311 documents\n",
      "    validation:\n",
      "     document indices in [1185311, 1247761) total of 62450 documents\n",
      "    test:\n",
      "     document indices in [1247761, 1249010) total of 1249 documents\n",
      " > loading doc-idx mapping from ../dataset/SV/webnyheter2013_text_document_train_indexmap_101ns_512sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from ../dataset/SV/webnyheter2013_text_document_train_indexmap_101ns_512sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from ../dataset/SV/webnyheter2013_text_document_train_indexmap_101ns_512sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.005 seconds\n",
      "    total number of samples: 53948\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from ../dataset/SV/webnyheter2013_text_document_valid_indexmap_5146ns_512sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from ../dataset/SV/webnyheter2013_text_document_valid_indexmap_5146ns_512sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from ../dataset/SV/webnyheter2013_text_document_valid_indexmap_5146ns_512sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.003 seconds\n",
      "    total number of samples: 5695\n",
      "    total number of epochs: 2\n",
      " > loading doc-idx mapping from ../dataset/SV/webnyheter2013_text_document_test_indexmap_5146ns_512sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from ../dataset/SV/webnyheter2013_text_document_test_indexmap_5146ns_512sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from ../dataset/SV/webnyheter2013_text_document_test_indexmap_5146ns_512sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.003 seconds\n",
      "    total number of samples: 5192\n",
      "    total number of epochs: 91\n",
      "> building indices for blendable datasets ...\n",
      " > sample ratios:\n",
      "   dataset 0, input: 1, achieved: 1\n",
      "> elapsed time for building blendable dataset indices: 0.00 (sec)\n",
      "> building indices for blendable datasets ...\n",
      " > sample ratios:\n",
      "   dataset 0, input: 1, achieved: 1\n",
      "> elapsed time for building blendable dataset indices: 0.00 (sec)\n",
      "> building indices for blendable datasets ...\n",
      " > sample ratios:\n",
      "   dataset 0, input: 1, achieved: 1\n",
      "> elapsed time for building blendable dataset indices: 0.00 (sec)\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2021-08-30 08:59:32 \n",
      "done with setup ...\n",
      "training ...\n",
      "time (ms) | model-and-optimizer-setup: 6065.80 | train/valid/test-data-iterators-setup: 2661.91\n",
      "[after training is done] datetime: 2021-08-30 08:59:32 \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      " validation loss at the end of training for val data | lm loss value: 1.081321E+01 | lm loss PPL: 4.967259E+04 | \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating iter 10/10\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      " validation loss at the end of training for test data | lm loss value: 1.081394E+01 | lm loss PPL: 4.970880E+04 | \n",
      "-------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!bash ./Megatron-LM/profile_SVGPT_BIG.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-right",
   "metadata": {},
   "source": [
    "## Remember to copy and paste your output on Slack or Zoom\n",
    "## Congratulations on completing the mission !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-worthy",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "\n",
    "## Licensing \n",
    "\n",
    "This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
