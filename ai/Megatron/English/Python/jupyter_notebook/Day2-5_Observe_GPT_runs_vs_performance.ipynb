{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "turned-producer",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "# 5 Monitor GPT training performance with varying config\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "- **The goal of this lab is to monitor the performance of your training runs with different GPT training configurations **\n",
    "    - motivation : why should we care ? \n",
    "    \n",
    "    Answer : bad config result in very low / inconsistent gpus utilizations which in turn, slow down training and therefore longer experiments per run, it's a lose-lose-lose situation on all sides.\n",
    "    ![see example](./Megatron-LM/pics/naive_run.JPG)\n",
    "    \n",
    "    - intro to profiling \n",
    "    - run profiling scripts \n",
    "    \n",
    "   \n",
    "    - example : naive run vs. improved run \n",
    "        - starts with multiGPUs \n",
    "    - exercise : beat the record !\n",
    "\n",
    "it is possible to obtain more than 90% GPU utilizations overall with high tensorcore ops sustained throughout during **training** for all gpus \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-ceiling",
   "metadata": {},
   "source": [
    "----------------------------------------------------------\n",
    "### intro to profiling \n",
    "\n",
    "#### NVIDIA Profiling ToolChain\n",
    "<center><img src=\"./Megatron-LM/pics/NVprofilingToolchain.JPG\" width=\"800\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-product",
   "metadata": {},
   "source": [
    "----------------------------------------------------------\n",
    "### The Profiling Workflow\n",
    "\n",
    "<center><img src=\"./Megatron-LM/pics/profiling_workflow.JPG\" width=\"700\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-brother",
   "metadata": {},
   "source": [
    "----------------------------------------------------------\n",
    "### Understanding Megatron training launches\n",
    "\n",
    "            ------------ call out terminals : watch -n 1 nvidia-smi to monitor training ------------------------\n",
    "<center><img src=\"./Megatron-LM/pics/Alt_callout2terminals.JPG\" width=\"600\"/></center>\n",
    "\n",
    "\n",
    "            -------- launch profiling sessions to record: visualize on Nsight( please use Nsight Systems version >=2021.3.1 ) ---------\n",
    "<center><img src=\"./Megatron-LM/pics/multigpu_naive_run.jpg\" width=\"1000\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-drill",
   "metadata": {},
   "source": [
    "---\n",
    "### install nvtx for annotation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "norwegian-fault",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nvtx in /home/zcharpy/.local/lib/python3.8/site-packages (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install nvtx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-shift",
   "metadata": {},
   "source": [
    "---\n",
    "### Let's first verify training works properly, \n",
    "modify your configuration and the number of GPUs available to you\n",
    "\n",
    "training output should look simialr to the following \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cross-ribbon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "using world size: 4, data-parallel-size: 4, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "setting global batch size to 4\n",
      "using torch.float16 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  checkpoint_activations .......................... True\n",
      "  checkpoint_num_layers ........................... 1\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 4\n",
      "  data_path ....................................... ['../dataset/EN/NVblogs_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  encoder_seq_length .............................. 512\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 100\n",
      "  eval_iters ...................................... 10\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  ffn_hidden_size ................................. 4096\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ True\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  global_batch_size ............................... 4\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 1024\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 64\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./Megatron-LM/sv_ckpt/\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. None\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 512\n",
      "  merge_file ...................................... ../dataset/EN/50k/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 1\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 24\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float16\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./Megatron-LM/sv_ckpt/\n",
      "  save_interval ................................... 100\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 512\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... None\n",
      "  train_samples ................................... 100\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_ddp ................... False\n",
      "  use_cpu_initialization .......................... None\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... ../dataset/EN/50k/gpt2-vocab.json\n",
      "  weight_decay .................................... 1e-05\n",
      "  world_size ...................................... 4\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "> compiling dataset index builder ...\n",
      "make: Entering directory '/home/zcharpy/bootcamp/jupyter_notebook/Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/zcharpy/bootcamp/jupyter_notebook/Megatron-LM/megatron/data'\n",
      ">>> done with dataset index builder. Compilation time: 0.051 seconds\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/zcharpy/bootcamp/jupyter_notebook/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/zcharpy/bootcamp/jupyter_notebook/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/zcharpy/bootcamp/jupyter_notebook/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 7.910 seconds\n",
      "time to initialize megatron (seconds): -47.489\n",
      "[after megatron is initialized] datetime: 2021-08-27 02:05:20 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 354347008\n",
      "setting training iterations to 25\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./Megatron-LM/sv_ckpt/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.21\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2021-08-27 02:05:20 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      100\n",
      "    validation: 40\n",
      "    test:       40\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.003578 seconds\n",
      "    number of documents: 74\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 70) total of 70 documents\n",
      "    validation:\n",
      "     document indices in [70, 74) total of 4 documents\n",
      "    test:\n",
      "     document indices in [74, 74) total of 0 documents\n",
      " > loading doc-idx mapping from ../dataset/EN/NVblogs_text_document_train_indexmap_100ns_512sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from ../dataset/EN/NVblogs_text_document_train_indexmap_100ns_512sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from ../dataset/EN/NVblogs_text_document_train_indexmap_100ns_512sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.011 seconds\n",
      "    total number of samples: 142\n",
      "    total number of epochs: 1\n",
      " > WARNING: could not find index map files, building the indices on rank 0 ...\n",
      " > last epoch number of samples (2) is smaller than 80% of number of samples per epoch (7), setting separate_last_epoch to True\n",
      " > elasped time to build and save doc-idx mapping (seconds): 0.083597\n",
      "    using:\n",
      "     number of documents:       4\n",
      "     number of epochs:          6\n",
      "     sequence length:           512\n",
      "     total number of samples:   46\n",
      " > elasped time to build and save sample-idx mapping (seconds): 0.057963\n",
      " > building shuffle index with split [0, 38) and [38, 46) ...\n",
      " > elasped time to build and save shuffle-idx mapping (seconds): 0.001258\n",
      " > loading doc-idx mapping from ../dataset/EN/NVblogs_text_document_valid_indexmap_40ns_512sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from ../dataset/EN/NVblogs_text_document_valid_indexmap_40ns_512sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from ../dataset/EN/NVblogs_text_document_valid_indexmap_40ns_512sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.003 seconds\n",
      "    total number of samples: 47\n",
      "    total number of epochs: 6\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2021-08-27 02:05:24 \n",
      "done with setup ...\n",
      "training ...\n",
      "time (ms) | model-and-optimizer-setup: 412.03 | train/valid/test-data-iterators-setup: 3889.83\n",
      "[before the start of training step] datetime: 2021-08-27 02:05:24 \n",
      " iteration       10/      25 | consumed samples:           40 | elapsed time per iteration (ms): 886.5 | learning rate: 0.000E+00 | global batch size:     4 | loss scale: 8388608.0 | number of skipped iterations:  10 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 454.99 | backward-compute: 286.45 | backward-params-all-reduce: 99.90 | backward-embedding-all-reduce: 0.05 | optimizer-copy-to-main-grad: 10.63 | optimizer-unscale-and-check-inf: 33.28 | optimizer: 44.16 | batch-generator: 5.34\n",
      " iteration       20/      25 | consumed samples:           80 | elapsed time per iteration (ms): 397.8 | learning rate: 1.422E-04 | global batch size:     4 | lm loss: 1.036185E+01 | loss scale: 131072.0 | grad norm: 3.873 | number of skipped iterations:   6 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 114.06 | backward-compute: 188.29 | backward-params-all-reduce: 53.84 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 5.99 | optimizer-unscale-and-check-inf: 11.17 | optimizer-clip-main-grad: 4.06 | optimizer-copy-main-to-model-params: 3.40 | optimizer: 39.04 | batch-generator: 4.50\n",
      "[Rank 0] (after 20 iterations) memory (MB) | allocated: 6758.6376953125 | max allocated: 7631.0029296875 | reserved: 9166.0 | max reserved: 9166.0\n",
      "[after training is done] datetime: 2021-08-27 02:05:40 \n",
      "saving checkpoint at iteration      25 to ./Megatron-LM/sv_ckpt/\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      " validation loss at the end of training for val data | lm loss value: 8.901263E+00 | lm loss PPL: 7.341241E+03 | \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "  successfully saved checkpoint at iteration      25 to ./Megatron-LM/sv_ckpt/\n"
     ]
    }
   ],
   "source": [
    "!bash ./Megatron-LM/verify_GPT3_Svenska.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "failing-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./Megatron-LM/sv_ckpt/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-territory",
   "metadata": {},
   "source": [
    "---\n",
    "## making sure the previous ran and saved ckpt are empty \n",
    "otherwise the model won't train if already reached specified --train-samples / --train-iter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "earlier-pontiac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -fr ./Megatron-LM/sv_ckpt/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-columbus",
   "metadata": {},
   "source": [
    "----------------------------------------------------------\n",
    "### My very first profiling session - naive run\n",
    "\n",
    "Let's launch a naive training run \n",
    "\n",
    "a successful profiling session should look something similar to the following output ---\n",
    "\n",
    "        ------------------------------------------------------------------------------------------------------------------\n",
    "          successfully saved checkpoint at iteration      12 to ./Megatron-LM/sv_ckpt/\n",
    "        *****************************************\n",
    "        Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune \n",
    "        the variable for optimal performance in your application as needed. \n",
    "        *****************************************\n",
    "        Processing events...\n",
    "        Capturing symbol files...\n",
    "        Saving temporary \"/tmp/nsys-report-84a0-cf36-0eed-f814.qdstrm\" file to disk...\n",
    "        Creating final output files...\n",
    "\n",
    "        Processing [==============================================================100%]\n",
    "        Saved report file to \"/tmp/nsys-report-84a0-cf36-0eed-f814.qdrep\"\n",
    "        Report file moved to \"/home/zcharpy/profiles/DLprof/naive/nsys_naive.qdrep\"\n",
    "\n",
    "             \n",
    "              \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "preceding-trainer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Initializing NVTX monkey patches\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "Done with NVTX monkey patching\n",
      "Initializing NVTX monkey patches\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "Done with NVTX monkey patching\n",
      "Initializing NVTX monkey patches\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "Done with NVTX monkey patching\n",
      "Initializing NVTX monkey patches\n",
      "Initializing NVTX monkey patches\n",
      "Initializing NVTX monkey patches\n",
      "Initializing NVTX monkey patches\n",
      "Initializing NVTX monkey patches\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "Done with NVTX monkey patching\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "Done with NVTX monkey patching\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "Done with NVTX monkey patching\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "Done with NVTX monkey patching\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "Done with NVTX monkey patching\n",
      "using world size: 8, data-parallel-size: 8, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  checkpoint_activations .......................... True\n",
      "  checkpoint_num_layers ........................... 1\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 8\n",
      "  data_path ....................................... ['../dataset/EN/NVblogs_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  encoder_seq_length .............................. 512\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 100\n",
      "  eval_iters ...................................... 10\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  ffn_hidden_size ................................. 4096\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  global_batch_size ............................... 8\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 1024\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 64\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./Megatron-LM/sv_ckpt/\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. None\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 512\n",
      "  merge_file ...................................... ../dataset/EN/50k/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 1\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 16\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./Megatron-LM/sv_ckpt/\n",
      "  save_interval ................................... 100\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 512\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... None\n",
      "  train_samples ................................... 100\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_ddp ................... False\n",
      "  use_cpu_initialization .......................... None\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... ../dataset/EN/50k/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 8\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "> compiling dataset index builder ...\n",
      "make: Entering directory '/home/zcharpy/bootcamp/jupyter_notebook/Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/zcharpy/bootcamp/jupyter_notebook/Megatron-LM/megatron/data'\n",
      ">>> done with dataset index builder. Compilation time: 0.645 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/zcharpy/bootcamp/jupyter_notebook/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/zcharpy/bootcamp/jupyter_notebook/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/zcharpy/bootcamp/jupyter_notebook/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 27.695 seconds\n",
      "time to initialize megatron (seconds): 40.267\n",
      "[after megatron is initialized] datetime: 2021-08-27 01:45:28 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 253577216\n",
      "setting training iterations to 12\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./Megatron-LM/sv_ckpt/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 44.04\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2021-08-27 01:45:29 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      100\n",
      "    validation: 80\n",
      "    test:       80\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.007752 seconds\n",
      "    number of documents: 74\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 70) total of 70 documents\n",
      "    validation:\n",
      "     document indices in [70, 74) total of 4 documents\n",
      "    test:\n",
      "     document indices in [74, 74) total of 0 documents\n",
      " > loading doc-idx mapping from ../dataset/EN/NVblogs_text_document_train_indexmap_100ns_512sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from ../dataset/EN/NVblogs_text_document_train_indexmap_100ns_512sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from ../dataset/EN/NVblogs_text_document_train_indexmap_100ns_512sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.032 seconds\n",
      "    total number of samples: 142\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from ../dataset/EN/NVblogs_text_document_valid_indexmap_80ns_512sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from ../dataset/EN/NVblogs_text_document_valid_indexmap_80ns_512sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from ../dataset/EN/NVblogs_text_document_valid_indexmap_80ns_512sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.008 seconds\n",
      "    total number of samples: 86\n",
      "    total number of epochs: 11\n",
      "> finished creating GPT datasets ...\n",
      "time (ms) | model-and-optimizer-setup: 1263.62 | train/valid/test-data-iterators-setup: 10291.63\n",
      "[after dataloaders are built] datetime: 2021-08-27 01:45:40 \n",
      "done with setup ...\n",
      "training ...\n",
      "[before the start of training step] datetime: 2021-08-27 01:45:40 \n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 3869.28369140625 | max allocated: 5229.60595703125 | reserved: 7306.0 | max reserved: 7306.0\n",
      " iteration       10/      12 | consumed samples:           80 | elapsed time per iteration (ms): 4807.7 | learning rate: 2.363E-05 | global batch size:     8 | lm loss: 9.601698E+00 | loss scale: 1.0 | grad norm: 1.856 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 2453.82 | backward-compute: 1445.84 | backward-params-all-reduce: 628.04 | backward-embedding-all-reduce: 0.32 | optimizer: 271.18 | batch-generator: 114.35\n",
      "[after training is done] datetime: 2021-08-27 01:46:35 \n",
      "saving checkpoint at iteration      12 to ./Megatron-LM/sv_ckpt/\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      " validation loss at the end of training for val data | lm loss value: 8.891883E+00 | lm loss PPL: 7.272700E+03 | \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "  successfully saved checkpoint at iteration      12 to ./Megatron-LM/sv_ckpt/\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-84a0-cf36-0eed-f814.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-84a0-cf36-0eed-f814.qdrep\"\n",
      "Report file moved to \"/home/zcharpy/profiles/DLprof/naive/nsys_naive.qdrep\"\n"
     ]
    }
   ],
   "source": [
    "!bash ./Megatron-LM/profile_naive_run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-drive",
   "metadata": {},
   "source": [
    "--------------------------------------------------\n",
    "-----\n",
    "visualizing the profiles via nsight should look similar to the following \n",
    "\n",
    "![multigpus naive run](./Megatron-LM/pics/multigpu_naive_run.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-belgium",
   "metadata": {},
   "source": [
    "---\n",
    "## below is a ReRun cell to experiment training configurations\n",
    "before each re-run, make sure you clear the checkpoint directory below \n",
    "<a id=\"Rerun_Cell\"></a>\n",
    "a successful profiling session should look like the following \n",
    "\n",
    "                training ...\n",
    "                time (ms) | model-and-optimizer-setup: 3900.44 | train/valid/test-data-iterators-setup: 3056.78\n",
    "                [after training is done] datetime: 2021-08-27 01:51:24 \n",
    "                ------------------------------------------------------------------------------------------------------------------\n",
    "                 validation loss at the end of training for val data | lm loss value: 1.099207E+01 | lm loss PPL: 5.940106E+04 | \n",
    "                ------------------------------------------------------------------------------------------------------------------\n",
    "                Processing events...\n",
    "                Capturing symbol files...\n",
    "                Saving temporary \"/tmp/nsys-report-7b95-50de-7e4d-bd7e.qdstrm\" file to disk...\n",
    "                Creating final output files...\n",
    "\n",
    "                Processing [==============================================================100%]\n",
    "                Saved report file to \"/tmp/nsys-report-7b95-50de-7e4d-bd7e.qdrep\"\n",
    "                Report file moved to \"/home/zcharpy/profiles/DLprof/2ndrun/nsys_improved.qdrep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "advanced-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -fr ./Megatron-LM/sv_ckpt/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "collect-ladder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Initializing NVTX monkey patches\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "Done with NVTX monkey patching\n",
      "Initializing NVTX monkey patches\n",
      "Initializing NVTX monkey patches\n",
      "Initializing NVTX monkey patches\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "Done with NVTX monkey patching\n",
      "Initializing NVTX monkey patches\n",
      "Initializing NVTX monkey patches\n",
      "Initializing NVTX monkey patches\n",
      "Initializing NVTX monkey patches\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "Done with NVTX monkey patching\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "Done with NVTX monkey patching\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "Done with NVTX monkey patching\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "Done with NVTX monkey patching\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "Done with NVTX monkey patching\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "Done with NVTX monkey patching\n",
      "using world size: 8, data-parallel-size: 1, tensor-model-parallel size: 8, pipeline-model-parallel size: 1 \n",
      "using torch.float16 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  checkpoint_activations .......................... True\n",
      "  checkpoint_num_layers ........................... 1\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['../dataset/EN/NVblogs_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  encoder_seq_length .............................. 512\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 200\n",
      "  eval_iters ...................................... 10\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  ffn_hidden_size ................................. 4096\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ True\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  global_batch_size ............................... 512\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 1024\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 32\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ./Megatron-LM/sv_ckpt/\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. None\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 512\n",
      "  merge_file ...................................... ../dataset/EN/50k/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 64\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 32\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float16\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ./Megatron-LM/sv_ckpt/\n",
      "  save_interval ................................... 100\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 512\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 8\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... None\n",
      "  train_samples ................................... 100\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_ddp ................... False\n",
      "  use_cpu_initialization .......................... None\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... ../dataset/EN/50k/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 8\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 8\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 943 dummy tokens (new size: 51200)\n",
      "> initializing torch distributed ...\n",
      "> initializing tensor model parallel with size 8\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "> compiling dataset index builder ...\n",
      "make: Entering directory '/home/zcharpy/bootcamp/jupyter_notebook/Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/home/zcharpy/bootcamp/jupyter_notebook/Megatron-LM/megatron/data'\n",
      ">>> done with dataset index builder. Compilation time: 0.613 seconds\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/zcharpy/bootcamp/jupyter_notebook/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/zcharpy/bootcamp/jupyter_notebook/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/zcharpy/bootcamp/jupyter_notebook/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 28.199 seconds\n",
      "time to initialize megatron (seconds): 3.584\n",
      "[after megatron is initialized] datetime: 2021-08-27 01:51:15 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 57636864\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 57636864\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (6, 0): 57636864\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 57636864 > number of parameters on (tensor, pipeline) model parallel rank (7, 0): 57636864\n",
      "\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 57636864\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (5, 0): 57636864\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (4, 0): 57636864\n",
      "setting training iterations to 0\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ./Megatron-LM/sv_ckpt/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 2.42\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2021-08-27 01:51:19 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      100\n",
      "    validation: 5120\n",
      "    test:       5120\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.004346 seconds\n",
      "    number of documents: 74\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 70) total of 70 documents\n",
      "    validation:\n",
      "     document indices in [70, 74) total of 4 documents\n",
      "    test:\n",
      "     document indices in [74, 74) total of 0 documents\n",
      " > loading doc-idx mapping from ../dataset/EN/NVblogs_text_document_train_indexmap_100ns_512sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from ../dataset/EN/NVblogs_text_document_train_indexmap_100ns_512sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from ../dataset/EN/NVblogs_text_document_train_indexmap_100ns_512sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.005 seconds\n",
      "    total number of samples: 142\n",
      "    total number of epochs: 1\n",
      " > loading doc-idx mapping from ../dataset/EN/NVblogs_text_document_valid_indexmap_5120ns_512sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from ../dataset/EN/NVblogs_text_document_valid_indexmap_5120ns_512sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from ../dataset/EN/NVblogs_text_document_valid_indexmap_5120ns_512sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.035 seconds\n",
      "    total number of samples: 5122\n",
      "    total number of epochs: 660\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2021-08-27 01:51:24 \n",
      "done with setup ...\n",
      "training ...\n",
      "time (ms) | model-and-optimizer-setup: 3900.44 | train/valid/test-data-iterators-setup: 3056.78\n",
      "[after training is done] datetime: 2021-08-27 01:51:24 \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      " validation loss at the end of training for val data | lm loss value: 1.099207E+01 | lm loss PPL: 5.940106E+04 | \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-7b95-50de-7e4d-bd7e.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-7b95-50de-7e4d-bd7e.qdrep\"\n",
      "Report file moved to \"/home/zcharpy/profiles/DLprof/2ndrun/nsys_improved.qdrep\"\n"
     ]
    }
   ],
   "source": [
    "!bash ./Megatron-LM/profile_2nd_run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-preliminary",
   "metadata": {},
   "source": [
    "--------------------------------------------------\n",
    "visualizing the profiles via nsight should look similar to the following \n",
    "\n",
    "![multigpus 2nd run](./Megatron-LM/pics/2ndrun.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-warning",
   "metadata": {},
   "source": [
    "<a id=\"TheChallenge\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-composition",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "## **Challenge ** - the best profile\n",
    "- prerequisites : \n",
    "        - use your current given # of gpus \n",
    "        - do NOT changing the following parameters --train-samples 100 \n",
    "        - you cannot go OOM \n",
    "        - you must sustain >80% GPUs utilization in the **training** phase \n",
    "        - training run must be finished and checkpoint must be saved successfully\n",
    "    - task : \n",
    "            given the above constraints, get as good training GPUs utilizations as possible\n",
    "    - Pass : sustain 80% gpus utils ( across all gpus) in the **training** phase !\n",
    " \n",
    "\n",
    "\n",
    "task: modify the [profiling bash script](./Megatron-LM/profile_2nd_run.sh) and rerun \n",
    "<a href=\"./Day2-5_Observe_GPT_runs_vs_performance.ipynb#Rerun_Cell\">Jump to ReRun Cell</a> \n",
    "monitor the training runs to get an overall >80% gpu utils in **training** runs \n",
    "\n",
    "```\n",
    "    TENSOR_MP_SIZE=\n",
    "    PIPELINE_MP_SIZE=\n",
    "\n",
    "    #GPT Config \n",
    "    LAYERS= \n",
    "    HIDDEN_SIZE=\n",
    "    ATTN_HEADS=\n",
    "    MICRO_BZ=\n",
    "    GB_BZ=\n",
    "    SEQ_LEN=\n",
    "    MAX_POS_EM=\n",
    "``` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-basic",
   "metadata": {},
   "source": [
    "---\n",
    "## Congratulations you are done for the day !\n",
    "## Back To [start menu](../Start_Here.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-assumption",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "\n",
    "## Licensing \n",
    "\n",
    "This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
