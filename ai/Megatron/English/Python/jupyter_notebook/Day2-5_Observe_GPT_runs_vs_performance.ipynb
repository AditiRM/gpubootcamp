{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "worth-conversation",
   "metadata": {},
   "source": [
    "# Monitor GPT training performance with varying config\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "The goal of this lab is to monitor the performance of your training runs with different GPT training configurations \n",
    "    - motivation : why should we care ? \n",
    "    \n",
    "    Answer : bad config result in very low / inconsistent gpus utilizations which in turn, slow down training and therefore longer experiments per run, it's a lose-lose-lose situation on all sides.\n",
    "    ![see example](./Megatron-LM/pics/naive_run.JPG)\n",
    "    \n",
    "    - intro to profiling \n",
    "    - run profiling scripts \n",
    "    \n",
    "   \n",
    "    - example : naive run vs. improved run \n",
    "        - starts with multiGPUs \n",
    "    - challenge : beat the record !\n",
    "\n",
    "Note: it is possible to obtain more than 80% GPU utilizations overall with high tensorcore ops sustained throughout during **training** for all gpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-penny",
   "metadata": {},
   "source": [
    "----------------------------------------------------------\n",
    "### Intro to profiling \n",
    "\n",
    "\n",
    "<center><img src=\"./Megatron-LM/pics/NVprofilingToolchain.JPG\" width=\"800\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-candle",
   "metadata": {},
   "source": [
    "----------------------------------------------------------\n",
    "### The Profiling Workflow\n",
    "\n",
    "<center><img src=\"./Megatron-LM/pics/profiling_workflow.JPG\" width=\"700\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-pizza",
   "metadata": {},
   "source": [
    "----------------------------------------------------------\n",
    "### Understanding Megatron training launches\n",
    "\n",
    "            ------------ call out terminals : watch -n 1 nvidia-smi to monitor training ------------------------\n",
    "<center><img src=\"./Megatron-LM/pics/Alt_callout2terminals.JPG\" width=\"600\"/></center>\n",
    "\n",
    "\n",
    "         ----- launch profiling sessions to record: visualize on Nsight( please use Nsight Systems version >=2021.4.1 ) ----\n",
    "         \n",
    "[Installing Nsight](https://developer.nvidia.com/gameworksdownload#?dn=nsight-systems-2021-4-1)\n",
    "\n",
    "[User Guide](https://docs.nvidia.com/nsight-systems/UserGuide/index.html)\n",
    "\n",
    "<center><img src=\"./Megatron-LM/pics/multigpu_naive_run.jpg\" width=\"1000\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-chambers",
   "metadata": {},
   "source": [
    "---\n",
    "### install nvtx for annotation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "false-imagination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nvtx\n",
      "  Downloading nvtx-0.2.3-cp38-cp38-manylinux2010_x86_64.whl (183 kB)\n",
      "\u001b[K     |████████████████████████████████| 183 kB 16.4 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: nvtx\n",
      "Successfully installed nvtx-0.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install nvtx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-uncertainty",
   "metadata": {},
   "source": [
    "---\n",
    "### Let's first verify training works properly, \n",
    "modify your configuration and the number of GPUs available to you\n",
    "\n",
    "training output should look simialr to the following \n",
    "\n",
    "                training ...\n",
    "                time (ms) | model-and-optimizer-setup: 412.03 | train/valid/test-data-iterators-setup: 3889.83\n",
    "                [before the start of training step] datetime: 2021-08-27 02:05:24 \n",
    "                 iteration       10/      25 | consumed samples:  40 | elapsed time per iteration (ms): 886.5 | \n",
    "                 learning rate:   0.000E+00 | global batch size: 4 | loss scale: 8388608.0 | number of skipped iterations:10 | \n",
    "                \n",
    "                ...\n",
    "                \n",
    "                [after training is done] datetime: 2021-08-27 02:05:40 \n",
    "                saving checkpoint at iteration      25 to ./Megatron-LM/sv_ckpt/\n",
    "                successfully saved checkpoint at iteration      25 to ./Megatron-LM/sv_ckpt/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-narrative",
   "metadata": {},
   "source": [
    "---\n",
    "# modify the bash script if you are using customized path \n",
    "[open verify_GPT3_Svenska.sh](./Megatron-LM/verify_GPT3_Svenska.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "large-saint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "using world size: 2, data-parallel-size: 2, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "setting global batch size to 2\n",
      "using torch.float16 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  checkpoint_activations .......................... True\n",
      "  checkpoint_num_layers ........................... 1\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 2\n",
      "  data_path ....................................... ['../dataset/EN/NVblog_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  encoder_seq_length .............................. 512\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 100\n",
      "  eval_iters ...................................... 10\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  ffn_hidden_size ................................. 4096\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ True\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  global_batch_size ............................... 2\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 1024\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 64\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ../sv_ckpt/\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. None\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 512\n",
      "  merge_file ...................................... ../dataset/EN/50k/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 1\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 24\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float16\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ../sv_ckpt/\n",
      "  save_interval ................................... 100\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 512\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... None\n",
      "  train_samples ................................... 100\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_ddp ................... False\n",
      "  use_cpu_initialization .......................... None\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... ../dataset/EN/50k/gpt2-vocab.json\n",
      "  weight_decay .................................... 1e-05\n",
      "  world_size ...................................... 2\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "> compiling dataset index builder ...\n",
      "make: Entering directory '/proj/guest_at_nsc/users/zcharpy/gpubootcamp/ai/Megatron/English/Python/jupyter_notebook/Megatron-LM/megatron/data'\n",
      "g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/opt/conda/include/python3.8 -I/opt/conda/lib/python3.8/site-packages/pybind11/include helpers.cpp -o helpers.cpython-38-x86_64-linux-gnu.so\n",
      "make: Leaving directory '/proj/guest_at_nsc/users/zcharpy/gpubootcamp/ai/Megatron/English/Python/jupyter_notebook/Megatron-LM/megatron/data'\n",
      ">>> done with dataset index builder. Compilation time: 15.129 seconds\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /proj/guest_at_nsc/users/zcharpy/gpubootcamp/ai/Megatron/English/Python/jupyter_notebook/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] c++ -MMD -MF scaled_upper_triang_masked_softmax.o.d -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1013\\\" -isystem /opt/conda/lib/python3.8/site-packages/torch/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -O3 -c /proj/guest_at_nsc/users/zcharpy/gpubootcamp/ai/Megatron/English/Python/jupyter_notebook/Megatron-LM/megatron/fused_kernels/scaled_upper_triang_masked_softmax.cpp -o scaled_upper_triang_masked_softmax.o \n",
      "[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output scaled_upper_triang_masked_softmax_cuda.cuda.o.d -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1013\\\" -isystem /opt/conda/lib/python3.8/site-packages/torch/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -gencode arch=compute_70,code=sm_70 --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -gencode arch=compute_80,code=sm_80 -std=c++14 -c /proj/guest_at_nsc/users/zcharpy/gpubootcamp/ai/Megatron/English/Python/jupyter_notebook/Megatron-LM/megatron/fused_kernels/scaled_upper_triang_masked_softmax_cuda.cu -o scaled_upper_triang_masked_softmax_cuda.cuda.o \n",
      "[3/3] c++ scaled_upper_triang_masked_softmax.o scaled_upper_triang_masked_softmax_cuda.cuda.o -shared -L/opt/conda/lib/python3.8/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o scaled_upper_triang_masked_softmax_cuda.so\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /proj/guest_at_nsc/users/zcharpy/gpubootcamp/ai/Megatron/English/Python/jupyter_notebook/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] c++ -MMD -MF scaled_masked_softmax.o.d -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1013\\\" -isystem /opt/conda/lib/python3.8/site-packages/torch/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -O3 -c /proj/guest_at_nsc/users/zcharpy/gpubootcamp/ai/Megatron/English/Python/jupyter_notebook/Megatron-LM/megatron/fused_kernels/scaled_masked_softmax.cpp -o scaled_masked_softmax.o \n",
      "[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output scaled_masked_softmax_cuda.cuda.o.d -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1013\\\" -isystem /opt/conda/lib/python3.8/site-packages/torch/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -gencode arch=compute_70,code=sm_70 --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -gencode arch=compute_80,code=sm_80 -std=c++14 -c /proj/guest_at_nsc/users/zcharpy/gpubootcamp/ai/Megatron/English/Python/jupyter_notebook/Megatron-LM/megatron/fused_kernels/scaled_masked_softmax_cuda.cu -o scaled_masked_softmax_cuda.cuda.o \n",
      "[3/3] c++ scaled_masked_softmax.o scaled_masked_softmax_cuda.cuda.o -shared -L/opt/conda/lib/python3.8/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o scaled_masked_softmax_cuda.so\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /proj/guest_at_nsc/users/zcharpy/gpubootcamp/ai/Megatron/English/Python/jupyter_notebook/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] c++ -MMD -MF layer_norm_cuda.o.d -DTORCH_EXTENSION_NAME=fused_mix_prec_layer_norm_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1013\\\" -isystem /opt/conda/lib/python3.8/site-packages/torch/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -O3 -c /proj/guest_at_nsc/users/zcharpy/gpubootcamp/ai/Megatron/English/Python/jupyter_notebook/Megatron-LM/megatron/fused_kernels/layer_norm_cuda.cpp -o layer_norm_cuda.o \n",
      "[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output layer_norm_cuda_kernel.cuda.o.d -DTORCH_EXTENSION_NAME=fused_mix_prec_layer_norm_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1013\\\" -isystem /opt/conda/lib/python3.8/site-packages/torch/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.8/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -gencode arch=compute_70,code=sm_70 --use_fast_math -maxrregcount=50 -gencode arch=compute_80,code=sm_80 -std=c++14 -c /proj/guest_at_nsc/users/zcharpy/gpubootcamp/ai/Megatron/English/Python/jupyter_notebook/Megatron-LM/megatron/fused_kernels/layer_norm_cuda_kernel.cu -o layer_norm_cuda_kernel.cuda.o \n",
      "[3/3] c++ layer_norm_cuda.o layer_norm_cuda_kernel.cuda.o -shared -L/opt/conda/lib/python3.8/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_mix_prec_layer_norm_cuda.so\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 122.935 seconds\n",
      "time to initialize megatron (seconds): 92.766\n",
      "[after megatron is initialized] datetime: 2021-09-15 09:53:00 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 354347008\n",
      "setting training iterations to 50\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ../sv_ckpt/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.42\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2021-09-15 09:53:01 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      100\n",
      "    validation: 20\n",
      "    test:       20\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.002436 seconds\n",
      "    number of documents: 71\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 67) total of 67 documents\n",
      "    validation:\n",
      "     document indices in [67, 71) total of 4 documents\n",
      "    test:\n",
      "     document indices in [71, 71) total of 0 documents\n",
      " > WARNING: could not find index map files, building the indices on rank 0 ...\n",
      " > only one epoch required, setting separate_last_epoch to False\n",
      " > elasped time to build and save doc-idx mapping (seconds): 0.001822\n",
      "    using:\n",
      "     number of documents:       67\n",
      "     number of epochs:          1\n",
      "     sequence length:           512\n",
      "     total number of samples:   144\n",
      " > elasped time to build and save sample-idx mapping (seconds): 0.002891\n",
      " > building shuffle index with split [0, 144) and [144, 144) ...\n",
      " > elasped time to build and save shuffle-idx mapping (seconds): 0.001165\n",
      " > loading doc-idx mapping from ../dataset/EN/NVblog_text_document_train_indexmap_100ns_512sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from ../dataset/EN/NVblog_text_document_train_indexmap_100ns_512sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from ../dataset/EN/NVblog_text_document_train_indexmap_100ns_512sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.003 seconds\n",
      "    total number of samples: 145\n",
      "    total number of epochs: 1\n",
      " > WARNING: could not find index map files, building the indices on rank 0 ...\n",
      " > last epoch number of samples (3) is larger than 80% of number of samples per epoch (4), setting separate_last_epoch to False\n",
      " > elasped time to build and save doc-idx mapping (seconds): 0.001544\n",
      "    using:\n",
      "     number of documents:       4\n",
      "     number of epochs:          5\n",
      "     sequence length:           512\n",
      "     total number of samples:   22\n",
      " > elasped time to build and save sample-idx mapping (seconds): 0.001337\n",
      " > building shuffle index with split [0, 22) and [22, 22) ...\n",
      " > elasped time to build and save shuffle-idx mapping (seconds): 0.001232\n",
      " > loading doc-idx mapping from ../dataset/EN/NVblog_text_document_valid_indexmap_20ns_512sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from ../dataset/EN/NVblog_text_document_valid_indexmap_20ns_512sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from ../dataset/EN/NVblog_text_document_valid_indexmap_20ns_512sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 23\n",
      "    total number of epochs: 5\n",
      "> finished creating GPT datasets ...\n",
      "time (ms) | model-and-optimizer-setup: 316.36 | train/valid/test-data-iterators-setup: 897.32\n",
      "[after dataloaders are built] datetime: 2021-09-15 09:53:02 \n",
      "done with setup ...\n",
      "training ...\n",
      "[before the start of training step] datetime: 2021-09-15 09:53:02 \n",
      " iteration       10/      50 | consumed samples:           20 | elapsed time per iteration (ms): 1702.8 | learning rate: 0.000E+00 | global batch size:     2 | loss scale: 8388608.0 | number of skipped iterations:  10 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1585.31 | backward-compute: 79.27 | backward-params-all-reduce: 12.81 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 7.90 | optimizer-unscale-and-check-inf: 15.73 | optimizer: 24.82 | batch-generator: 14.40\n",
      " iteration       20/      50 | consumed samples:           40 | elapsed time per iteration (ms): 90.2 | learning rate: 1.491E-04 | global batch size:     2 | lm loss: 1.060733E+01 | loss scale: 65536.0 | number of skipped iterations:   7 | number of nan iterations:   0 |\n",
      "[Rank 0] (after 20 iterations) memory (MB) | allocated: 6758.6376953125 | max allocated: 7631.0029296875 | reserved: 9166.0 | max reserved: 9166.0\n",
      "time (ms) | forward-compute: 17.93 | backward-compute: 41.76 | backward-params-all-reduce: 11.86 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 3.40 | optimizer-unscale-and-check-inf: 2.74 | optimizer-clip-main-grad: 2.63 | optimizer-copy-main-to-model-params: 1.15 | optimizer: 16.77 | batch-generator: 0.74\n",
      " iteration       30/      50 | consumed samples:           60 | elapsed time per iteration (ms): 88.2 | learning rate: 1.291E-04 | global batch size:     2 | lm loss: 9.023698E+00 | loss scale: 65536.0 | grad norm: 1.795 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 18.13 | backward-compute: 35.19 | backward-params-all-reduce: 11.34 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 3.39 | optimizer-unscale-and-check-inf: 2.72 | optimizer-clip-main-grad: 4.75 | optimizer-copy-main-to-model-params: 3.57 | optimizer: 22.88 | batch-generator: 0.75\n",
      " iteration       40/      50 | consumed samples:           80 | elapsed time per iteration (ms): 88.2 | learning rate: 8.996E-05 | global batch size:     2 | lm loss: 8.049081E+00 | loss scale: 65536.0 | grad norm: 1.597 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 18.08 | backward-compute: 35.34 | backward-params-all-reduce: 11.36 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 3.39 | optimizer-unscale-and-check-inf: 2.74 | optimizer-clip-main-grad: 4.74 | optimizer-copy-main-to-model-params: 3.57 | optimizer: 22.85 | batch-generator: 0.74\n",
      " iteration       50/      50 | consumed samples:          100 | elapsed time per iteration (ms): 88.0 | learning rate: 4.694E-05 | global batch size:     2 | lm loss: 7.628845E+00 | loss scale: 65536.0 | grad norm: 1.615 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 17.78 | backward-compute: 35.44 | backward-params-all-reduce: 11.30 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 3.39 | optimizer-unscale-and-check-inf: 2.73 | optimizer-clip-main-grad: 4.73 | optimizer-copy-main-to-model-params: 3.58 | optimizer: 22.84 | batch-generator: 0.77\n",
      "[after training is done] datetime: 2021-09-15 09:53:22 \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      " validation loss at the end of training for val data | lm loss value: 7.841995E+00 | lm loss PPL: 2.545278E+03 | \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "saving checkpoint at iteration      50 to ../sv_ckpt/\n",
      "  successfully saved checkpoint at iteration      50 to ../sv_ckpt/\n",
      "./Megatron-LM/verify_GPT3_Svenska.sh: line 51: nt-activations: command not found\n"
     ]
    }
   ],
   "source": [
    "!bash ./Megatron-LM/verify_GPT3_Svenska.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cognitive-harrison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_0000050  latest_checkpointed_iteration.txt\n"
     ]
    }
   ],
   "source": [
    "!ls ../sv_ckpt/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-genesis",
   "metadata": {},
   "source": [
    "---\n",
    "## making sure the previous ran and saved ckpt are empty \n",
    "otherwise the model won't train if already reached specified --train-samples / --train-iter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "delayed-renewal",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -fr ../sv_ckpt/*\n",
    "!rm -fr ../dataset/EN/*.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-credits",
   "metadata": {},
   "source": [
    "----------------------------------------------------------\n",
    "### My very first profiling session - naive run\n",
    "\n",
    "Let's launch a naive training run \n",
    "\n",
    "a successful profiling session should look something similar to the following output ---\n",
    "\n",
    "        ------------------------------------------------------------------------------------------------------------------\n",
    "          successfully saved checkpoint at iteration      12 to ./Megatron-LM/sv_ckpt/\n",
    "        *****************************************\n",
    "        Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune \n",
    "        the variable for optimal performance in your application as needed. \n",
    "        *****************************************\n",
    "        Processing events...\n",
    "        Capturing symbol files...\n",
    "        Saving temporary \"/tmp/nsys-report-84a0-cf36-0eed-f814.qdstrm\" file to disk...\n",
    "        Creating final output files...\n",
    "\n",
    "        Processing [==============================================================100%]\n",
    "        Saved report file to \"/tmp/nsys-report-84a0-cf36-0eed-f814.qdrep\"\n",
    "        Report file moved to \"/home/zcharpy/profiles/DLprof/naive/nsys_naive.qdrep\"\n",
    "\n",
    "             \n",
    "              \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-acrobat",
   "metadata": {},
   "source": [
    "---\n",
    "# modify the naive run bash script directly \n",
    "[open profile_naive_run.sh ](./Megatron-LM/profile_naive_run.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "serious-finance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "Initializing NVTX monkey patchesInitializing NVTX monkey patches\n",
      "\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "Done with NVTX monkey patching\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "Done with NVTX monkey patching\n",
      "using world size: 2, data-parallel-size: 2, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  checkpoint_activations .......................... True\n",
      "  checkpoint_num_layers ........................... 1\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 2\n",
      "  data_path ....................................... ['../dataset/EN/NVblog_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  encoder_seq_length .............................. 512\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 100\n",
      "  eval_iters ...................................... 10\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  ffn_hidden_size ................................. 4096\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  global_batch_size ............................... 8\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 1024\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 64\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ../sv_ckpt/\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. None\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 512\n",
      "  merge_file ...................................... ../dataset/EN/50k/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 1\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 16\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ../sv_ckpt/\n",
      "  save_interval ................................... 100\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 512\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... None\n",
      "  train_samples ................................... 100\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_ddp ................... False\n",
      "  use_cpu_initialization .......................... None\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... ../dataset/EN/50k/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 2\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 4\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)\n",
      "> initializing torch distributed ...\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "> compiling dataset index builder ...\n",
      "make: Entering directory '/proj/guest_at_nsc/users/zcharpy/gpubootcamp/ai/Megatron/English/Python/jupyter_notebook/Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/proj/guest_at_nsc/users/zcharpy/gpubootcamp/ai/Megatron/English/Python/jupyter_notebook/Megatron-LM/megatron/data'\n",
      ">>> done with dataset index builder. Compilation time: 0.432 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /proj/guest_at_nsc/users/zcharpy/gpubootcamp/ai/Megatron/English/Python/jupyter_notebook/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /proj/guest_at_nsc/users/zcharpy/gpubootcamp/ai/Megatron/English/Python/jupyter_notebook/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /proj/guest_at_nsc/users/zcharpy/gpubootcamp/ai/Megatron/English/Python/jupyter_notebook/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.096 seconds\n",
      "time to initialize megatron (seconds): 22.970\n",
      "[after megatron is initialized] datetime: 2021-09-15 10:17:26 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 253577216\n",
      "setting training iterations to 12\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ../sv_ckpt/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 1.16\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2021-09-15 10:17:27 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      100\n",
      "    validation: 80\n",
      "    test:       80\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.002909 seconds\n",
      "    number of documents: 71\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 67) total of 67 documents\n",
      "    validation:\n",
      "     document indices in [67, 71) total of 4 documents\n",
      "    test:\n",
      "     document indices in [71, 71) total of 0 documents\n",
      " > WARNING: could not find index map files, building the indices on rank 0 ...\n",
      " > only one epoch required, setting separate_last_epoch to False\n",
      " > elasped time to build and save doc-idx mapping (seconds): 0.002079\n",
      "    using:\n",
      "     number of documents:       67\n",
      "     number of epochs:          1\n",
      "     sequence length:           512\n",
      "     total number of samples:   144\n",
      " > elasped time to build and save sample-idx mapping (seconds): 0.005640\n",
      " > building shuffle index with split [0, 144) and [144, 144) ...\n",
      " > elasped time to build and save shuffle-idx mapping (seconds): 0.001334\n",
      " > loading doc-idx mapping from ../dataset/EN/NVblog_text_document_train_indexmap_100ns_512sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from ../dataset/EN/NVblog_text_document_train_indexmap_100ns_512sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from ../dataset/EN/NVblog_text_document_train_indexmap_100ns_512sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.003 seconds\n",
      "    total number of samples: 145\n",
      "    total number of epochs: 1\n",
      " > WARNING: could not find index map files, building the indices on rank 0 ...\n",
      " > last epoch number of samples (1) is smaller than 80% of number of samples per epoch (4), setting separate_last_epoch to True\n",
      " > elasped time to build and save doc-idx mapping (seconds): 0.003275\n",
      "    using:\n",
      "     number of documents:       4\n",
      "     number of epochs:          19\n",
      "     sequence length:           512\n",
      "     total number of samples:   84\n",
      " > elasped time to build and save sample-idx mapping (seconds): 0.002993\n",
      " > building shuffle index with split [0, 79) and [79, 84) ...\n",
      " > elasped time to build and save shuffle-idx mapping (seconds): 0.001204\n",
      " > loading doc-idx mapping from ../dataset/EN/NVblog_text_document_valid_indexmap_80ns_512sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from ../dataset/EN/NVblog_text_document_valid_indexmap_80ns_512sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from ../dataset/EN/NVblog_text_document_valid_indexmap_80ns_512sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 85\n",
      "    total number of epochs: 19\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2021-09-15 10:17:28 \n",
      "done with setup ...\n",
      "training ...\n",
      "time (ms) | model-and-optimizer-setup: 161.95 | train/valid/test-data-iterators-setup: 1224.16\n",
      "[before the start of training step] datetime: 2021-09-15 10:17:28 \n",
      " iteration       10/      12 | consumed samples:           80 | elapsed time per iteration (ms): 1558.8 | learning rate: 2.363E-05 | global batch size:     8 | lm loss: 9.580709E+00 | loss scale: 1.0 | grad norm: 2.024 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 816.38 | backward-compute: 600.18 | backward-params-all-reduce: 93.68 | backward-embedding-all-reduce: 0.22 | optimizer: 45.83 | batch-generator: 25.92\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 3869.28369140625 | max allocated: 5426.107421875 | reserved: 6534.0 | max reserved: 6534.0\n",
      "[after training is done] datetime: 2021-09-15 10:17:46 \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      " validation loss at the end of training for val data | lm loss value: 8.895156E+00 | lm loss PPL: 7.296543E+03 | \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "saving checkpoint at iteration      12 to ../sv_ckpt/\n",
      "  successfully saved checkpoint at iteration      12 to ../sv_ckpt/\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-4642-8c23-394b-8c2e.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-4642-8c23-394b-8c2e.qdrep\"\n",
      "Report file moved to \"/proj/guest_at_nsc/users/zcharpy/gpubootcamp/ai/Megatron/English/Python/jupyter_notebook/../profiles/naive/nsys_naive.qdrep\"\n"
     ]
    }
   ],
   "source": [
    "!bash ./Megatron-LM/profile_naive_run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-martial",
   "metadata": {},
   "source": [
    "--------------------------------------------------\n",
    "-----\n",
    "visualizing the profiles via nsight should look similar to the following \n",
    "<center><img src=\"./Megatron-LM/pics/GPUs_utils_naive.JPG\" width=\"1000\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-private",
   "metadata": {},
   "source": [
    "---\n",
    "## below is a ReRun cell to experiment training configurations\n",
    "before each re-run, make sure you clear the checkpoint directory below \n",
    "<a id=\"Rerun_Cell\"></a>\n",
    "a successful profiling session should look like the following \n",
    "\n",
    "                training ...\n",
    "                time (ms) | model-and-optimizer-setup: 3900.44 | train/valid/test-data-iterators-setup: 3056.78\n",
    "                [after training is done] datetime: 2021-08-27 01:51:24 \n",
    "                ------------------------------------------------------------------------------------------------------------------\n",
    "                 validation loss at the end of training for val data | lm loss value: 1.099207E+01 | lm loss PPL: 5.940106E+04 | \n",
    "                ------------------------------------------------------------------------------------------------------------------\n",
    "                Processing events...\n",
    "                Capturing symbol files...\n",
    "                Saving temporary \"/tmp/nsys-report-7b95-50de-7e4d-bd7e.qdstrm\" file to disk...\n",
    "                Creating final output files...\n",
    "\n",
    "                Processing [==============================================================100%]\n",
    "                Saved report file to \"/tmp/nsys-report-7b95-50de-7e4d-bd7e.qdrep\"\n",
    "                Report file moved to \"/home/zcharpy/profiles/DLprof/2ndrun/nsys_improved.qdrep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "architectural-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -fr ../sv_ckpt/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-astronomy",
   "metadata": {},
   "source": [
    "---\n",
    "# modify the profile_2nd_run.sh directly\n",
    "[open profile_2nd_run.sh](./Megatron-LM/profile_2nd_run.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "varied-breakdown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "Initializing NVTX monkey patchesInitializing NVTX monkey patches\n",
      "\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "Done with NVTX monkey patching\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "Done with NVTX monkey patching\n",
      "using world size: 2, data-parallel-size: 1, tensor-model-parallel size: 2, pipeline-model-parallel size: 1 \n",
      "using torch.float16 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  checkpoint_activations .......................... True\n",
      "  checkpoint_num_layers ........................... 1\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... ['../dataset/EN/NVblog_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  encoder_seq_length .............................. 1024\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 200\n",
      "  eval_iters ...................................... 10\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  ffn_hidden_size ................................. 8192\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ True\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  global_batch_size ............................... 128\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 2048\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 64\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ ../sv_ckpt/\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 10\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. None\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  merge_file ...................................... ../dataset/EN/50k/gpt2-merges.txt\n",
      "  micro_batch_size ................................ 16\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 32\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 32\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float16\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ ../sv_ckpt/\n",
      "  save_interval ................................... 100\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 1024\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 2\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. GPT2BPETokenizer\n",
      "  train_iters ..................................... None\n",
      "  train_samples ................................... 100\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_ddp ................... False\n",
      "  use_cpu_initialization .......................... None\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... ../dataset/EN/50k/gpt2-vocab.json\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 2\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 8\n",
      "> building GPT2BPETokenizer tokenizer ...\n",
      " > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)\n",
      "> initializing torch distributed ...\n",
      "> initializing tensor model parallel with size 2\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "> compiling dataset index builder ...\n",
      "make: Entering directory '/proj/guest_at_nsc/users/zcharpy/gpubootcamp/ai/Megatron/English/Python/jupyter_notebook/Megatron-LM/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/proj/guest_at_nsc/users/zcharpy/gpubootcamp/ai/Megatron/English/Python/jupyter_notebook/Megatron-LM/megatron/data'\n",
      ">>> done with dataset index builder. Compilation time: 0.460 seconds\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /proj/guest_at_nsc/users/zcharpy/gpubootcamp/ai/Megatron/English/Python/jupyter_notebook/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /proj/guest_at_nsc/users/zcharpy/gpubootcamp/ai/Megatron/English/Python/jupyter_notebook/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /proj/guest_at_nsc/users/zcharpy/gpubootcamp/ai/Megatron/English/Python/jupyter_notebook/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 2.971 seconds\n",
      "time to initialize megatron (seconds): 2.770\n",
      "[after megatron is initialized] datetime: 2021-09-16 19:18:58 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 859672576\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 859672576\n",
      "setting training iterations to 0\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file ../sv_ckpt/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 3.02\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2021-09-16 19:18:59 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      100\n",
      "    validation: 1280\n",
      "    test:       1280\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.005245 seconds\n",
      "    number of documents: 71\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 67) total of 67 documents\n",
      "    validation:\n",
      "     document indices in [67, 71) total of 4 documents\n",
      "    test:\n",
      "     document indices in [71, 71) total of 0 documents\n",
      " > WARNING: could not find index map files, building the indices on rank 0 ...\n",
      " > last epoch number of samples (28) is smaller than 80% of number of samples per epoch (72), setting separate_last_epoch to True\n",
      " > elasped time to build and save doc-idx mapping (seconds): 0.002224\n",
      "    using:\n",
      "     number of documents:       67\n",
      "     number of epochs:          2\n",
      "     sequence length:           1024\n",
      "     total number of samples:   144\n",
      " > elasped time to build and save sample-idx mapping (seconds): 0.004913\n",
      " > building shuffle index with split [0, 72) and [72, 144) ...\n",
      " > elasped time to build and save shuffle-idx mapping (seconds): 0.002444\n",
      " > loading doc-idx mapping from ../dataset/EN/NVblog_text_document_train_indexmap_100ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from ../dataset/EN/NVblog_text_document_train_indexmap_100ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from ../dataset/EN/NVblog_text_document_train_indexmap_100ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 145\n",
      "    total number of epochs: 2\n",
      " > WARNING: could not find index map files, building the indices on rank 0 ...\n",
      " > last epoch number of samples (1) is larger than 80% of number of samples per epoch (2), setting separate_last_epoch to False\n",
      " > elasped time to build and save doc-idx mapping (seconds): 0.002341\n",
      "    using:\n",
      "     number of documents:       4\n",
      "     number of epochs:          579\n",
      "     sequence length:           1024\n",
      "     total number of samples:   1281\n",
      " > elasped time to build and save sample-idx mapping (seconds): 0.001686\n",
      " > building shuffle index with split [0, 1281) and [1281, 1281) ...\n",
      " > elasped time to build and save shuffle-idx mapping (seconds): 0.002335\n",
      " > loading doc-idx mapping from ../dataset/EN/NVblog_text_document_valid_indexmap_1280ns_1024sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from ../dataset/EN/NVblog_text_document_valid_indexmap_1280ns_1024sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from ../dataset/EN/NVblog_text_document_valid_indexmap_1280ns_1024sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 1282\n",
      "    total number of epochs: 579\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2021-09-16 19:19:01 \n",
      "done with setup ...\n",
      "time (ms) | model-and-optimizer-setup: 772.93 | train/valid/test-data-iterators-setup: 1032.39\n",
      "training ...\n",
      "[after training is done] datetime: 2021-09-16 19:19:01 \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      " validation loss at the end of training for val data | lm loss value: 1.126569E+01 | lm loss PPL: 7.809596E+04 | \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-3aa1-f1a6-09c2-c853.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-3aa1-f1a6-09c2-c853.qdrep\"\n",
      "Report file moved to \"/proj/guest_at_nsc/users/zcharpy/gpubootcamp/ai/Megatron/English/Python/jupyter_notebook/../profiles/2ndrun/nsys_improved.qdrep\"\n"
     ]
    }
   ],
   "source": [
    "!bash ./Megatron-LM/profile_2nd_run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-leisure",
   "metadata": {},
   "source": [
    "--------------------------------------------------\n",
    "visualizing the profiles via nsight should look similar to the following \n",
    "<center><img src=\"./Megatron-LM/pics/gpus_utils_improved.JPG\" width=\"1000\"/></center>\n",
    "<center><img src=\"./Megatron-LM/pics/2ndrun.JPG\" width=\"1000\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-channel",
   "metadata": {},
   "source": [
    "<a id=\"TheChallenge\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-consideration",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "## **Challenge ** - the best profile\n",
    "- prerequisites : \n",
    "        - use your current given # of gpus \n",
    "        - do NOT changing the following parameters --train-samples 100 \n",
    "        - you cannot go OOM \n",
    "        - you must sustain >60% GPUs utilization in the **training** phase \n",
    "        - training run must be finished and checkpoint must be saved successfully\n",
    "    - task : \n",
    "            given the above constraints, get as good training GPUs utilizations as possible\n",
    "    - Pass : sustain 60% gpus utils ( across all gpus) in the **training** phase !\n",
    " \n",
    "\n",
    "\n",
    "task: modify this --> [profiling bash script](./Megatron-LM/profile_2nd_run.sh) and rerun \n",
    "<a href=\"./Day2-5_Observe_GPT_runs_vs_performance.ipynb#Rerun_Cell\">GO to ReRun Cell</a> \n",
    "monitor the training runs to get an overall >80% gpu utils in **training** runs \n",
    "\n",
    "```\n",
    "    TENSOR_MP_SIZE=\n",
    "    PIPELINE_MP_SIZE=\n",
    "\n",
    "    #GPT Config \n",
    "    LAYERS= \n",
    "    HIDDEN_SIZE=\n",
    "    ATTN_HEADS=\n",
    "    MICRO_BZ=\n",
    "    GB_BZ=\n",
    "    SEQ_LEN=\n",
    "    MAX_POS_EM=\n",
    "``` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-overview",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "NVIDIA Nsight Systems : https://docs.nvidia.com/nsight-systems/index.html\n",
    "\n",
    "NVTX Tutorial : https://developer.nvidia.com/blog/nvidia-tools-extension-api-nvtx-annotation-tool-for-profiling-code-in-python-and-c-c/\n",
    "\n",
    "Nsight Systems : https://developer.nvidia.com/blog/transitioning-nsight-systems-nvidia-visual-profiler-nvprof/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-baking",
   "metadata": {},
   "source": [
    "---\n",
    "## Congratulations you are done for the day !\n",
    "## Back To [start menu](../Start_Here.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-birmingham",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "\n",
    "## Licensing \n",
    "\n",
    "This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
