{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&ensp;\n",
    "[Home Page](../../Start_Here.ipynb)\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&ensp;\n",
    "[1]\n",
    "[2](../diffusion_1d/Diffusion_Problem_Notebook.ipynb)\n",
    "[3](../spring_mass/Spring_Mass_Problem_Notebook.ipynb)\n",
    "[4](../chip_2d/Challenge_CFD_Problem_Notebook.ipynb)\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "[Next Notebook](../diffusion_1d/Diffusion_Problem_Notebook.ipynb)\n",
    "\n",
    "\n",
    "# Introduction to Physics Informed Neural Networks (PINNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will see the advantages of Physics Informed modeling over data-driven modeling and will also outline the brief theory about Physics Informed Neural Networks (PINNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINNs vs Data-Driven Methods\n",
    "\n",
    "The data-driven methods are very useful in applications where the data is already available to learn from, or in situations where the underlying physics is not known. For such methods, the generalization errors are dependent on the amount of training data. Usually, obtaining the training data from simulations is a resource-hungry process and can limit the amounts of data the model is trained on. This places restrictions on the quality of results obtained from the data-driven methods and it usually takes a large amount of training data and fine-tuning of the model to capture the physics better.\n",
    "\n",
    "<img src=\"image_data_driven_cons.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "Since for most of the physical systems, we know the underlying governing equations (e.g. Navier Stokes equations for the fluid mechanics problems), we can make the neural networks more cognizant of these physical laws by incorporating these governing equations in training framework. In such cases we can train the models on lesser data, or even train completely without any data from a solver. Also, in such Physics-Informed Neural Networks, treatment of inverse problems and parameterized problems (e.g. problems with parameterized geometry/material properties/boundary conditions) is easy. Due to these advantages, we will now see how to apply PINNs for physics-based problems using the SimNet library. \n",
    "\n",
    "Let's start with discussing the theory behind the PINNs briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Solver Methodology \n",
    "\n",
    "In this section we provide a brief introduction to solving differential equations with neural networks. The idea is to use a neural network to approximate the solution to the given differential equation and boundary conditions. We train this neural network by constructing a loss function for how well the neural network is satisfying the differential equation and boundary conditions. If the network is able to minimize this loss function then it will in effect, solve the given differential equation.\n",
    "\n",
    "To illustrate this idea we will give an example of solving the following problem,\n",
    "\n",
    "\\begin{equation} \\label{1d_equation}\n",
    "    \\mathbf{P} : \\left\\{\\begin{matrix}\n",
    "\\frac{\\delta^2 u}{\\delta x^2}(x) = f(x), \\\\ \n",
    "\\\\\n",
    "u(0) = u(1) = 0,\n",
    "\\end{matrix}\\right.\n",
    "\\end{equation}\n",
    "\n",
    "We start by constructing a neural network $u_{net}(x)$. The input to this network is a single value $x \\in \\mathbb{R}$ and its output is also a single value $u_{net}(x) \\in \\mathbb{R}$. We suppose that this neural network is infinitely differentiable, $u_{net} \\in C^{\\infty}$. The typical neural network used is a deep fully connected network where the activation functions are infinitely differentiable. \n",
    "\n",
    "Next we need to construct a loss function to train this neural network. We easily encode the boundary conditions as a loss in the following way:\n",
    "\n",
    "\\begin{equation}\n",
    "  L_{BC} = u_{net}(0)^2 + u_{net}(1)^2\n",
    "\\end{equation}\n",
    "\n",
    "For encoding the equations, we need to compute the derivatives of $u_{net}$. Using automatic differentiation we can do so and compute $\\frac{\\delta^2 u_{net}}{\\delta x^2}(x)$. This allows us to write a loss function of the form:\n",
    "\n",
    "\\begin{equation} \\label{sumation_loss}\n",
    "  L_{residual} = \\frac{1}{N}\\sum^{N}_{i=0} \\left( \\frac{\\delta^2 u_{net}}{\\delta x^2}(x_i) - f(x_i) \\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "Where the $x_i$'s are a batch of points sampled in the interior, $x_i \\in (0, 1)$. Our total loss is then $L = L_{BC} + L_{residual}$. Optimizers such as Adam are used to train this neural network. Given $f(x)=1$, the true solution is $\\frac{1}{2}(x-1)x$. Upon solving the problem, you can obtain good agreement between the exact solution and the neural network solution as shown in Figure below.\n",
    "\n",
    "<img src=\"single_parabola.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parmeterized Problems\n",
    "\n",
    "One important advantage of a neural network solver over traditional numerical methods is its ability to solve parameterized geometries. To illustrate this concept we solve a parameterized version of the above problem. Suppose we want to know how the solution to this equation changes as we move the position on the boundary condition $u(l)=0$. We can parameterize this position with a variable $l \\in [1,2]$ and our equation now has the form,\n",
    "\n",
    "\\begin{equation} \\label{1d_equation2}\n",
    "    \\mathbf{P} : \\left\\{\\begin{matrix}\n",
    "\\frac{\\delta^2 u}{\\delta x^2}(x) = f(x), \\\\ \n",
    "\\\\\n",
    "u(0) = u(l) = 0,\n",
    "\\end{matrix}\\right.\n",
    "\\end{equation}\n",
    "\n",
    "To solve this parameterized problem we can have the neural network take $l$ as input, $u_{net}(x,l)$. The losses then take the form,\n",
    "\n",
    "\\begin{equation}\n",
    "  L_{residual} = \\int_1^2 \\int_0^l \\left( \\frac{\\delta^2 u_{net}}{\\delta x^2}(x,l) - f(x) \\right)^2 dx dl \\approx \\left(\\int_1^2 \\int^l_0 dxdl\\right) \\frac{1}{N} \\sum^{N}_{i=0} \\left(\\frac{\\delta^2 u_{net}}{\\delta x^2}(x_i, l_i) - f(x_i)\\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "  L_{BC} = \\int_1^2 (u_{net}(0,l))^2 + (u_{net}(l,l) dl \\approx \\left(\\int_1^2 dl\\right) \\frac{1}{N} \\sum^{N}_{i=0} (u_{net}(0, l_i))^2 + (u_{net}(l_i, l_i))^2\n",
    "\\end{equation}\n",
    "\n",
    "In figure below we see the solution to the differential equation for various $l$ values after optimizing the network on this loss. While this example problem is overly simplistic, the ability to solve parameterized geometries presents significant industrial value. Instead of performing a single simulation we can solve multiple designs at the same time and for reduced computational cost. More examples of this can be found in *SimNet User Guide Chapter 13*.\n",
    "\n",
    "<img src=\"every_parabola.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Problems \n",
    "\n",
    "Another useful application of a neural network solver is solving inverse problems. In an inverse problem, we start with a set of observations and then use those observations to calculate the causal factors that produced them. To illustrate how to solve inverse problems with a neural network solver, we give the example of inverting out the source term $f(x)$ from same equation from above problem. Suppose we are given the solution $u_{true}(x)$ at 100 random points between 0 and 1 and we want to determine the $f(x)$ that is causing it. We can do this by making two neural networks $u_{net}(x)$ and $f_{net}(x)$ to approximate both $u(x)$ and $f(x)$. These networks are then optimized to minimize the following losses;\n",
    "\n",
    "\\begin{equation}\n",
    "  L_{residual} \\approx \\left(\\int^1_0 dx\\right) \\frac{1}{N} \\sum^{N}_{i=0} \\left(\\frac{\\delta^2 u_{net}}{\\delta x^2}(x_i, l_i) - f_{net}(x_i)\\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "  L_{data} = \\frac{1}{100} \\sum^{100}_{i=0} (u_{net}(x_i) - u_{true}(x_i))^2\n",
    "\\end{equation}\n",
    "\n",
    "Using the function $u_{true}(x)=\\frac{1}{48} (8 x (-1 + x^2) - (3 sin(4 \\pi x))/\\pi^2)$ the solution for $f(x)$ is $x + sin(4 \\pi x)$. We solve this problem and compare the results in Figures below.\n",
    "\n",
    "Comparison of true solution for $f(x)$ and the function approximated by the NN:\n",
    "<img src=\"inverse_parabola.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Comparison of $u_{net}(x)$ and the train points from $u_{true}$:\n",
    "<img src=\"inverse_parabola_2.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "More examples of solving an inverse problem can be found in the *SimNet User Guide Chapter 12*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame, display\n",
    "#filepath = \"http://wikipedia.org\" # works with websites too!\n",
    "filepath = \"SimNet_v21.06_User_Guide.pdf\"\n",
    "IFrame(filepath, width=700, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Licensing\n",
    "This material is released by NVIDIA Corporation under the Creative Commons Attribution 4.0 International (CC BY 4.0)\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&ensp;\n",
    "[Home Page](../../Start_Here.ipynb)\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&ensp;\n",
    "[1]\n",
    "[2](../diffusion_1d/Diffusion_Problem_Notebook.ipynb)\n",
    "[3](../spring_mass/Spring_Mass_Problem_Notebook.ipynb)\n",
    "[4](../chip_2d/Challenge_CFD_Problem_Notebook.ipynb)\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "[Next Notebook](../diffusion_1d/Diffusion_Problem_Notebook.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
