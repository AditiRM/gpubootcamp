{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79103e8e-da47-4528-ac30-2cdd2adccaea",
   "metadata": {},
   "source": [
    "Before we begin, let's get an overview of the CUDA driver version and the GPUs running on the server by executing the `nvidia-smi` command below. Highlight the cell below by clicking on it and then either hit `Ctrl+Enter` on the keyboard or click on the `Run` button on the toolbar above. The output will be visible below the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca7ab3b-aef8-41d6-a568-8458bce7c7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ca37c-9955-4b7a-83ff-c99761bb51e9",
   "metadata": {},
   "source": [
    "# The NVIDIA Collectives Communications Library (NCCL)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "NCCL (pronounced “Nickel”) is a library of multi-GPU collective and point-to-point communication primitives that are topology-aware and are designed to be light-weight, depending only on the standard Okay, Okay, C++ and CUDA libraries. NCCL can be deployed in single-process or multi-process applications, handling required inter-process communication transparently. Moreover, the API is quite similar to MPI and the most-used MPI primitives.\n",
    "\n",
    "In general, NCCL is optimized for high bandwidth and low latency over PCIe and NVLink high speed interconnect for intra-node communication and sockets and InfiniBand for inter-node communication. NCCL—allows CUDA applications and DL frameworks in particular—to efficiently use multiple GPUs without having to implement complex communication algorithms and adapt them to every platform.\n",
    "\n",
    "**Relevance to Deep Learning:** The NVIDIA AI libraries in CUDA-X depend on NCCL to provide a programming abstraction that is highly tuned for each platform and topology-aware through advanced topology detection, generic path search, and algorithms optimized for NVIDIA architectures. Consequently, developers using deep learning frameworks can rely on NCCL’s highly optimized, MPI compatible and topology aware routines, to take full advantage of all available GPUs within and across multiple nodes.\n",
    "\n",
    "#### NCCL compared with MPI\n",
    "\n",
    "Here are some differenciating factors of NCCL compared to CUDA-aware MPI:\n",
    "\n",
    "* NCCL APIs are initiated from the CPU, but they execute on the GPU and they move or exchange data among GPU memories whereas MPI executes entirely on CPU. \n",
    "* It also uses CUDA Stream semantics with a stream parameter while MPI (CUDA-aware or otherwise) is not stream-aware.\n",
    "* NCCL requires a parent communication framework like MPI or SHMEM. \n",
    "* Unlike MPI, it does not have tags.\n",
    "* NCCL is most optimized for collective communication and is more efficient than MPI in dense systems like DGX.\n",
    "\n",
    "#### Workflow and APIs\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
