{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd0ae66a",
   "metadata": {},
   "source": [
    "Before we begin, let's get an overview of the CUDA driver version and the GPUs running on the server by executing the `nvidia-smi` command below. Highlight the cell below by clicking on it and then either hit `Ctrl+Enter` on the keyboard or click on the `Run` button on the toolbar above. The output will be visible below the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d483e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ddba18",
   "metadata": {},
   "source": [
    "# Learning Objectives\n",
    "\n",
    "In this tutorial, the goal is to:\n",
    "* Parallelize the single-GPU code using CUDA Memcpy and streams\n",
    "* Understand intra-node topology and underlying technologies like GPUDirect P2P and their implication on program performance\n",
    "\n",
    "# Multi-GPU Programming\n",
    "\n",
    "In this section we first cover the principle behind decomposing data among the GPUs, known as domain decomposition. Then, we understand and implement the baseline multi-GPU code using `cudaSetDevice` and `cudaMemcpy` functions. \n",
    "\n",
    "### Domain Decomposition\n",
    "\n",
    "Before we begin, we define two important terms:\n",
    "\n",
    "* **Latency:** The amount of time it takes to take a unit of data from point A to point B. For example, if 4B of data can be transferred from point A to B in 4 $\\mu$s, that is the latency of transfer.\n",
    "* **Bandwidth:** The amount of data that can be transferred from point A to point B in a unit of time. For example, if the width of the bus is 64KiB and latency of transfer between point A and B is 4 $\\mu$s, the bandwidth is 64KiB * (1/4$\\mu$s) = 1.6 GiB/s.\n",
    "\n",
    "To parallelize our application to multi-GPUs, we first review the different methods of domain decomposition available to us for splitting the data among the GPUs, thereby distributing the work. Broadly, we can divide data into either stripes or tiles.\n",
    "\n",
    "* **Stripes**: They minimize the number of neighbours, require communication among less neighbours, and are optimal for latency bound communication.\n",
    "\n",
    "* **Tiles**: They minimize surface area/ volume ratio of the grid, require communicating less data, and are optimal for bandwidth bound communication.\n",
    "\n",
    "![domain_decomposition](../../images/domain_decomposition.png)\n",
    "\n",
    "When we divide the global grid between GPUs, only the boundaries of each GPU-local grid need to be communicated with the neighboring GPUs, as they need the updated grid-point values for the next iteration. Therefore, we use horizontal stripes (as C/ C++ are row-major) in our tutorials for domain decomposition, enabling data parallelism.\n",
    "\n",
    "### Halo Exchange\n",
    "\n",
    "We term the exchange of top and bottom rows after each iterations the \"halo exchange\". Review the image below and notice that we update the topmost and bottomost rows of the grid to implement the periodic boundary condition. Recall that the left and right columns of the grid constitute Dirichlet boundary conditions (that is, constant value).\n",
    "\n",
    "![halo_exchange](../../images/halo_exchange.png)\n",
    "\n",
    "## CUDA concepts: Part 1\n",
    "\n",
    "### Setting the GPU\n",
    "\n",
    "To verify that our system has multiple GPUs in each node, run the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49697bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d045bd",
   "metadata": {},
   "source": [
    "The command should output more than one GPU. Inside a program, the number of GPU in the node can be obtained using the `cudaGetDeviceCount(int *count)` function and to perform any task, like running a CUDA kernel, copy operation, etc. on a particular GPU, we use the `cudaSetDevice(int device)` function.\n",
    "\n",
    "### Copying between GPUs\n",
    "\n",
    "The `cudaMemcpy` function supports GPU to GPU copy using the `cudaMemcpyDeviceToDevice` flag and the source and destination memory addresses should reside in GPU devices. \n",
    "\n",
    "For example, if we want to copy 1000 floats from the array `arr_gpu_0` allocated on GPU 0 to the array `arr_gpu_1`, the function call is:\n",
    "\n",
    "```c\n",
    "cudaMemcpy(arr_gpu_1, arr_gpu_0, 1000 * sizeof(float), cudaMemcpyDeviceToDevice);\n",
    "```\n",
    "\n",
    "Recall that CUDA kernel calls made from the host are non-blocking (asynchronous) by default. That is, the control may return back to the host thread before the device kernel finishes execution. To perform the halo exchange, we need to perform copy operations between each GPU and its neighbours. However, for large copy sizes, `cudaMemcpy` is blocking with respect to the host. \n",
    "\n",
    "Thus, we cannot use the following code snippet:\n",
    "\n",
    "```c\n",
    "for (int i = 0; i < 2; i++) {\n",
    "    // Set current device\n",
    "    cudaSetDevice(i);\n",
    "    // Define row number of top and bottom neighbours, etc.\n",
    "    TopNeighbour = ...; BotNeighbour = ...; // and so-on\n",
    "    // Launch device kernel on GPU i\n",
    "    jacobi_kernel<<<dim_grid, dim_block>>>(...);\n",
    "    // Halo exchange\n",
    "    cudaMemcpy(grid_rows[TopNeighbour], grid_rows[myTop], size, cudaMemcpyDeviceToDevice);\n",
    "    cudaMemcpy(grid_rows[BotNeighbour], grid_rows[myBot], size, cudaMemcpyDeviceToDevice);\n",
    "    // Norm check, swapping current and previous grid arrays, etc.\n",
    "} // Serializes operations with respect to the host\n",
    "```\n",
    "\n",
    "As this code results in serialized execution:\n",
    "\n",
    "![memcpy_serialized](../../images/memcpy_serialized.png)\n",
    "\n",
    "### Asynchronous operations\n",
    "\n",
    "Instead of `cudaMemcpy`, we can use the `cudaMemcpyAsync` function which is asynchronous with respect to the host. This allows the host to launch device kernels and copy operations concurrently, enabling parallel execution across GPUs. \n",
    "\n",
    "The correct code snippet is as follows:\n",
    "\n",
    "```c\n",
    "for (int i = 0; i < 2; i++) {\n",
    "    // Set current device\n",
    "    cudaSetDevice(i);\n",
    "    // Launch device kernel on GPU i\n",
    "    jacobi_kernel<<<dim_grid, dim_block>>>(...);\n",
    "}\n",
    "for (int i = 0; i < 2; i++) {\n",
    "    // Define row number of top and bottom neighbours, etc.\n",
    "    TopNeighbour = ...; BotNeighbour = ...; // and so-on\n",
    "    // Halo exchange, notice the use of Async function\n",
    "    cudaMemcpyAsync(grid_rows[TopNeighbour], grid_rows[myTop], size, cudaMemcpyDeviceToDevice);\n",
    "    cudaMemcpyAsync(grid_rows[BotNeighbour], grid_rows[myBot], size, cudaMemcpyDeviceToDevice);\n",
    "    // Norm check, swapping current and previous grid arrays, etc.\n",
    "} // Parallel execution across multiple GPUs\n",
    "```\n",
    "\n",
    "And the execution time of the application is reduced:\n",
    "\n",
    "![memcpyasync_parallel](../../images/memcpyasync_parallel.png)\n",
    "\n",
    "## Implementation exercise: Part 1\n",
    "\n",
    "Now, let's parallelize our code across multiple GPUs by using `cudaSetDevice` and `cudaMemcpyAsync` operations. Open the [jacobi_memcpy.cu](../../source_code/memcpy/jacobi_memcpy.cu) file by using the `File` $\\rightarrow$ `Open...` option.\n",
    "\n",
    "Understand the flow of the program from within the `main` function. Review the following pre-Jacobi-computation steps:\n",
    "\n",
    "1. Computation of the memory chunk size to be allocated on each GPU stored in the `chunk_size` integer array.\n",
    "2. Allocation of memory on each GPU: Notice the use of array pointers like `a_new`, `l2_norm_d`, `iy_start`, etc. that point to device arrays allocated on GPU pointed to by `dev_id` variable.\n",
    "3. Initialization of Dirichlet boundary conditions on left and right boundaries.\n",
    "4. Share of initial top and bottom local grid-point values between neighbours.\n",
    "\n",
    "\n",
    "Now, within the iterative Jacobi loop (the `while` loop), implement the following marked as `TODO: Part 1-`:\n",
    "\n",
    "1. Set current GPU and call device kernel with correct device arrays in function arguments.\n",
    "2. Asynchronously copy GPU-local L2 norm back to CPU and implement top and bottom halo exchanges.\n",
    "3. Synchronize the devices at the end of each iteration using `cudaDeviceSynchronize` function.\n",
    "\n",
    "Review the topic on Asynchronous Operations above if in doubt. Recall the utility of using separate `for` loops for launching device kernels and initiating copy operations.\n",
    "\n",
    "After implementing these, let's compile the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6dc6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../../source_code/memcpy && make clean && make jacobi_memcpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698ab130",
   "metadata": {},
   "source": [
    "Ensure there are no compiler warnings or errors. Validate the implementation by running the binary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50debc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../../source_code/memcpy && ./jacobi_memcpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e276f70",
   "metadata": {},
   "source": [
    "The last couple of lines of the output will give the number and IDs of GPUs used, execution timings, speedup, and efficiency metrics. Review Metrics of Interest section in [single GPU overview](../single_gpu/single_gpu_overview.ipynb) tutorial for more information). We tested the code on a DGX-1 system with 8 Tesla V100 16GB GPUs, and we got the following output:\n",
    "\n",
    "```bash\n",
    "Num GPUs: 8. Using GPU ID: 0, 1, 2, 3, 4, 5, 6, 7, \n",
    "16384x16384: 1 GPU:   5.0272 s, 8 GPUs:   1.1376 s, speedup:     4.42, efficiency:    55.24\n",
    "```\n",
    "\n",
    "Notice that we got a speed-up of $4.42\\times$ using 8 GPUs and a corresponding efficiency of $55.24\\%$. The numbers will vary depending on number of available GPUs in your system, the communication topology, GPU type, etc.\n",
    "\n",
    "### Profiling\n",
    "\n",
    "Now, profile the execution with `nsys`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3187cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../../source_code/memcpy/ && nsys profile --trace=cuda,nvtx --stats=true -o jacobi_memcpy_sys_report --force-overwrite true ./jacobi_memcpy -gpus 0,7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ac727d",
   "metadata": {},
   "source": [
    "In the profiler timeline, the first few seconds denote the single-GPU code running on one of the GPUs. This version is executed so we can compare the multi-GPU version with it and we have already analyzed it. Let's analyze the multi-GPU timeline:\n",
    "\n",
    "IMAGE LINK HERE\n",
    "\n",
    "NSYS DESCRIPTION HERE\n",
    "\n",
    "The solution for this exercise is present in `source_code/memcpy/solution` directory: [jacobi_memcpy.cu](../../source_code/memcpy/solution/jacobi_memcpy.cu)\n",
    "\n",
    "## CUDA concepts: Part 2\n",
    "\n",
    "### Host Staging of Copy Operations\n",
    "\n",
    "Using `cudaMemcpyAsync` instead of `cudaMemcpy` allows us to issue copy and compute operations on multiple GPUs concurrently. The path taken by the data in both the cases is denoted by the red arrow as follows:\n",
    "\n",
    "![memcpy_host_staging](../../images/memcpy_host_staging.png)\n",
    "\n",
    "That is, in the GPU-to-GPU memory copy, the data traverses from GPU 0 the PCIe bus to the CPU, where it is staged in a buffer before being copied to GPU 1. This is called \"host staging\" and it decreases the bandwidth while increasing the latency of the operation. If we eliminate host staging, we can usually improve the performance of our application.\n",
    "\n",
    "### Peer-to-Peer Memory Access\n",
    "\n",
    "P2P allows devices to address each other's memory from within device kernels and eliminates host staging by transferring data either through the PCIe switch or through NVLink as denoted by the red arrow below. \n",
    "\n",
    "![memcpy_p2p_overview](../../images/memcpy_p2p_overview.png)\n",
    "\n",
    "Peer-to-Peer (P2P) memory access requires GPUs to share a Unified Virtual Address Space (UVA). UVA means that a single address space is used for the host and all modern NVIDIA GPU devices (specifically, those with compute capibility of 2.0 or higher).\n",
    "\n",
    "This P2P memory access feature is supported between two devices if `cudaDeviceCanAccessPeer()` returns true for these two devices. P2P must be enabled between two devices by calling `cudaDeviceEnablePeerAccess()` as illustrated in the following code sample:\n",
    "\n",
    "```c\n",
    "cudaSetDevice(currDevice);\n",
    "int canAccessPeer = 0;\n",
    "cudaDeviceCanAccessPeer(&canAccessPeer, currDevice, PeerDevice);\n",
    "if (canAccessPeer) {\n",
    "    cudaDeviceEnablePeerAccess(PeerDevice, 0);\n",
    "}\n",
    "```\n",
    "\n",
    "Note that this enables a unidirectional P2P access where `currDevice` can perform memory access to `PeerDevice`. If we want `PeerDevice` to be able to access `currDevice` via P2P, then we need to use the code accordingly.\n",
    "\n",
    "First, let's check if P2P is supported between the GPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f757d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi topo -p2p r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afbc209",
   "metadata": {},
   "source": [
    "The `topo` sub-command requests information on the GPU communication topology, `-p2p` flag requests P2P status, and `r` asks whether P2P reads are supported. Change `r` to `w` to check whether writes are supported. We share our output on a DGX-1 system with 8 Tesla V100s, focusing on the capabilities of GPU 0:\n",
    "\n",
    "![nvidia_smi_p2p_gpu0](../../images/nvidia_smi_p2p_gpu0.png)\n",
    "\n",
    "This means GPU 0 can communicate via P2P with GPUs 1 through 4. For GPUs 5 through 7, it must use host staging.\n",
    "\n",
    "To check whether P2P via NVLink is supported, run the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1250c02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi topo -p2p n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d84934b",
   "metadata": {},
   "source": [
    "In our DGX-1 system, the result is similar as before. Even if P2P via NVLink is not supported on your system, as long as `-p2p r` and `-p2p w` are supported between GPUs, P2P capability is available.\n",
    "\n",
    "## Implementation Exercise: Part 2\n",
    "\n",
    "Now, let us improve our program performance by enabling P2P access between GPUs, wherever possible. The `jacobi_memcpy.cu` code accepts a runtime argument `-p2p` which should enable P2P access between GPUs. \n",
    "\n",
    "Modify the code by searching for `TODO: Part 2` and enabling GPU `devices[dev_id]` to access peer GPUs `devices[top]` and `devices[bottom]`, whenever possible. \n",
    "\n",
    "Notice that the code snippet is within a `for` loop which sets and iterates over each GPU, which is why bidirectional P2P will be enabled. Take help from the code sample in the previous section.\n",
    "\n",
    "Now, let's compile the code again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e8da79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../../source_code/memcpy && make clean && make jacobi_memcpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd648c93",
   "metadata": {},
   "source": [
    "Ensure there are no compiler warnings or errors. Validate the implementation by running the binary with P2P enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed251978",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../../source_code/memcpy && ./jacobi_memcpy -p2p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acc2cc0",
   "metadata": {},
   "source": [
    "The output we got on our DGX-1 system is:\n",
    "\n",
    "```bash\n",
    "Num GPUs: 8. Using GPU ID: 0, 1, 2, 3, 4, 5, 6, 7, \n",
    "16384x16384: 1 GPU:   4.4487 s, 8 GPUs:   0.8798 s, speedup:     5.06, efficiency:    63.21 \n",
    "```\n",
    "\n",
    "Notice that the efficiency increased by about $8\\%$ to $63.21\\%$ compared to our baseline implementation. You can run the baseline again by removing the `-p2p` flag. Note that if P2P is not supported on your system, you will likely not experience any performance gain.\n",
    "\n",
    "### Profiling\n",
    "\n",
    "IMAGE LINK HERE\n",
    "\n",
    "NSYS DESCRIPTION HERE\n",
    "\n",
    "## Intra-Node Communication Topology\n",
    "\n",
    "Run the command below to display your node's GPU and NIC communication topology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be59a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi topo -m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81fa29e",
   "metadata": {},
   "source": [
    "If the output is unclear, you can launch a Terminal session by clicking on `File` $\\rightarrow$ Open and following the steps as shown:\n",
    "\n",
    "![open_terminal_session](../../images/open_terminal_session.png)\n",
    "\n",
    "On our DGX-1 system, the output is as follows:\n",
    "\n",
    "![nvidia_smi_topo_output](../../images/nvidia_smi_topo_output.png)\n",
    "\n",
    "Focus one a particular row, say GPU 0. The output states that GPUs 1 through 4 are connected to it via NVLink (in addition to PCIe) and GPUs 5 through 7 are connected to it via PCIe as well as an \"SMP\" interconnect. We have a dual-socket system and the CPUs in these sockets are connected by an interconnect known as SMP interconnect.\n",
    "\n",
    "Thus, GPU 0 to GPU 5 communication happens via not just PCIe, but also over the inter-socket interconnect within the same node. Clearly, this is a longer path than say the one between GPU 0 and GPU 1, which are connected via NVLink directly. We will discuss the NIC to GPU connection in the inter-node section of this bootcamp.\n",
    "\n",
    "Even within the GPUs connected via NVLink, we see different annotations such as `NV1` and `NV2` that affect the communication bandwidth and hence the performance. In this section, we will explore the nuances associated with a diverse intra-node GPU communication topology like in the output above. Specifically, in our system, the communication topology is as follows:\n",
    "\n",
    "![dgx1_8x_tesla_v100_topo](../../images/dgx1_8x_tesla_v100_topo.png)\n",
    "\n",
    "Qualitatively, the bandwidth and latency vary with the topology as follows:\n",
    "\n",
    "![intra_node_topology_map](../../images/intra_node_topology_map.png)\n",
    "\n",
    "Host staging implies traversing through the CPU and the travel path taken is one of PHB, NODE, and SYS. In contrast, if the path taken is either NV1, NV2, or PIX, then P2P is available. PXB implies that the GPUs belong to different PCIe hubs and P2P is usually not supported in this case.\n",
    "\n",
    "A double NVLink connection provides twice the bandwidth compared to a single NVLink. \n",
    "\n",
    "For a pair of 2 GPUs, the peak bidirectional bandwidth are as follows:\n",
    "* PCIe: Using PIX topology, 15.75GB/s for PCIe Gen 3.0 and 31.5GB/s for PCIe Gen 4.0.\n",
    "* NVLink: Using NV# topology, 50GB/s per connection. So a double NVLink connection has 100GB/s peak bidirectional bandwidth.\n",
    "\n",
    "Let us understand what difference the underlying communication topology can make to the application performance in the following sub-section.\n",
    "\n",
    "**Note:** If your command output doesn't show any NVLink connection or if there's no difference in connection type (PIX, PXB, PHB, NODE, SYS, NV#) between any 2 pair of GPUs, then the communication bandwidth and latency will likely be the same between any pair and the following sub-sections will not display any performance difference.\n",
    "\n",
    "### Performance variation due to system topology\n",
    "\n",
    "So far, the code runs the multi-GPU version on all available GPUs in a node (8 in our case). We can supply the `-gpus` runtime flag to the binary to run our code on specific GPUs. If we want to run on only 2 GPUs, namely GPU 0 and GPU 3, we use the `-gpus 0,3` argument. \n",
    "\n",
    "Try to find the GPU pair with highest bandwidth available as per the table above and replace `0,3` with those GPUs, and then run the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd50a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../../source_code/memcpy && ./jacobi_memcpy -p2p -gpus 0,7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c4eb06",
   "metadata": {},
   "source": [
    "The efficiency would likely be higher than before due to less inter-GPU communication (each GPU does more wok instead). Our output is as follows:\n",
    "\n",
    "```bash\n",
    "Num GPUs: 2. Using GPU ID: 0, 3, \n",
    "16384x16384: 1 GPU:   4.4513 s, 2 GPUs:   2.2664 s, speedup:     1.96, efficiency:    98.20  \n",
    "```\n",
    "\n",
    "Now, run the binary a pair of GPUs that have the lowest available bandwidth. In our case, we use GPU 0 and GPU 7. Our output is:\n",
    "\n",
    "```bash\n",
    "Num GPUs: 2. Using GPU ID: 0, 7, \n",
    "16384x16384: 1 GPU:   4.4529 s, 2 GPUs:   2.3454 s, speedup:     1.90, efficiency:    94.93  \n",
    "```\n",
    "\n",
    "Now remove the `-p2p` flag and run the command again for GPUs 0 and 7. We didn't get any difference in performance. As you may recall, P2P is not possible between GPUs 0 and 7, so the underlying communication path doesn't change, resulting in same performance with and without the `-p2p` flag. The same can be confirmed by profiling the application and looking at the operations performed in the Nsight Systems timeline. \n",
    "\n",
    "![p2p_2_gpu_memcpy_nsys](../../images/p2p_2_gpu_memcpy_nsys.png)\n",
    "\n",
    "Try a few other GPU combinations and toggle P2P so see if the performance variation correlates with the table above. Also try reducing the grid size using `-nx` and `-ny` flags (to say 8192$\\times$8192) and see the effect on efficiency. \n",
    "\n",
    "### Benchmarking the system topology\n",
    "\n",
    "Our application is not very memory intensive. As is visible from the profiler output, $\\gt95\\%$ of the time in GPU is spent on computation. Therefore, to get a quantitative measure of latency and bandwidth impact due to topology, we run a micro-benchmark.\n",
    "\n",
    "**The p2pBandwidthLatencyTest micro-benchmark**\n",
    "\n",
    "p2pBandwidthLatencyTest is a part of [CUDA Samples GitHub repository](https://github.com/NVIDIA/cuda-samples) available to help CUDA developers. \n",
    "\n",
    "As the name suggests, this test measures the bandwidth and latency impact of P2P and underlying communication topology. Let's compile the benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fa162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../../source_code/p2pBandwidthLatencyTest/ && make clean && make"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429bc0cf",
   "metadata": {},
   "source": [
    "Now, let's run the benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f607f88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../../source_code/p2pBandwidthLatencyTest/ && ./p2pBandwidthLatencyTest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacdaacc",
   "metadata": {},
   "source": [
    "The first part of the benchmark gives device information and P2P access available from each GPU (similar to `nvidia-smi topo -m` command). Next, the benchmark measures the unidirectional and bidirectional bandwidth and latency with P2P disabled and enabled.\n",
    "\n",
    "We share partial results obtained in our DGX-1 system:\n",
    "\n",
    "```bash\n",
    "Bidirectional P2P=Disabled Bandwidth Matrix (GB/s)\n",
    "   D\\D     0      1      2      3      4      5      6      7 \n",
    "     0 783.95   9.56  14.43  14.46  14.47  14.24  14.51  14.43 \n",
    "\n",
    "Bidirectional P2P=Enabled Bandwidth Matrix (GB/s)\n",
    "   D\\D     0      1      2      3      4      5      6      7 \n",
    "     0 784.87  48.49  48.49  96.85  96.90  14.25  14.54  14.49 \n",
    "     \n",
    "P2P=Disabled Latency Matrix (us)\n",
    "   GPU     0      1      2      3      4      5      6      7 \n",
    "     0   1.78  17.52  16.41  16.43  17.35  16.88  17.34  16.85 \n",
    "     \n",
    "P2P=Enabled Latency (P2P Writes) Matrix (us)\n",
    "   GPU     0      1      2      3      4      5      6      7 \n",
    "     0   1.76   1.62   1.61   2.01   2.02  18.44  19.15  19.34\n",
    "```\n",
    "\n",
    "Our system is based on PCIe gen 3.0 with a peak maximum GPU-GPU PCIe banwidth of 15.75 GB/s. Let us analyze and understand these results:\n",
    "\n",
    "* GPU 0 and GPU 1/2: Connected by a single NVLink connection. By enabling P2P-\n",
    "  - Bandwidth reaches close to the maximum peak of 50 GB/s.\n",
    "  - Latency decreases by an order of magnitude.\n",
    "* GPU 0 and GPU 3/4: Connected by a double NVLink connection. By enabling P2P-\n",
    "  - Bandwidth reaches close to the maximum peak of 100 GB/s.\n",
    "  - Latency decreases by an order of magnitude.\n",
    "* GPU 0 and GPU 5/6/7: Connected by PCIe and SMP interconnect. By enabling P2P- \n",
    "  - Bandwidth is unchanged.\n",
    "  - Latency increases a marginally.\n",
    "  \n",
    "Correlate these results with the communication topology that can be displayed by usng `nvidia-smi topo -m` command and the qualtitative table in the previous section. They should be consistent with one another.\n",
    "\n",
    "In general, we should try to set the GPUs in an application such that a GPU can share data with its neighbours using a high-bandwidth, low-latency communication topology. Enabling P2P, when possible, usually improves the performance by eliminating host staging."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
